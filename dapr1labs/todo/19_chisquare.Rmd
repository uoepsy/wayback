```{r, echo=FALSE}
HIDDEN_SOLS=FALSE
TOGGLE=TRUE
set.seed(15732)
ggplot2::theme_set(ggplot2::theme_gray(base_size=13))
```

```{r}
knitr::opts_chunk$set(fig.align='center', message=FALSE, out.width = '80%')
```

# Chi-square tests {#chap-chi-square}


<div class="lo">
#### Instructions {-}
  
- The lab material consists of a worked example for the walkthrough, and a series of questions for you to attempt.
- The Rmarkdown file for this week is [here](https://uoe-psychology.github.io/uoe_psystats/dapr1/labsheets/week_19_practice.Rmd).


#### Learning outcomes {-}

By the end of this lab, you should be able to:

**LO1.** Understand when to use a chi-square goodness-of-fit and chi-square test of independence.

**LO2.** Check the validity conditions.

**LO3.** Report the test results.

</div>



## Recap {-}

<font size="2">

| Test type | One-sample | Independent samples | Paired data |
|------------------------|:-------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------------------:|
| Null hypothesis | $H_0 : \mu = \mu_0$ | $H_0 : \mu_1 - \mu_2 = 0$ | $H_0 : \mu_d = 0$ |
| Alternative hypothesis | $$ \begin{array}{ll}   i.   & H_1 : \mu < \mu_0 \\   ii.  & H_1 : \mu > \mu_0 \\   iii. & H_1 : \mu \neq \mu_0 \end{array} $$ | $$ \begin{array}{ll}   i.   & H_1 : \mu_1 - \mu_2 < 0 \\   ii.  & H_1 : \mu_1 - \mu_2 > 0 \\   iii. & H_1 : \mu_1 - \mu_2 \neq 0 \end{array} $$ | $$ \begin{array}{ll}   i.   & H_1 : \mu_d < 0 \\   ii.  & H_1 : \mu_d > 0 \\   iii. & H_1 : \mu_d \neq 0 \end{array} $$ |
| Test statistic | $$ t = \frac{\bar x - \mu_0}{SE(\bar x)} $$ | $$ t = \frac{\bar x_1 - \bar x_2}{SE(\bar x_1 - \bar x_2)} $$ | $$ t = \frac{\bar d - 0}{SE(\bar d)} $$ |
| Standard error | $SE(\bar x) = \frac{s}{\sqrt{n}}$ | (a) Unequal population variances<br>$SE(\bar x_1 - \bar x_2) =  \sqrt{  \frac{s_1^2}{n_1} +   \frac{s_2^2}{n_2} }$<br><br>(b) Equal population variances<br>$SE(\bar x_1 - \bar x_2) =  s_p \sqrt{  \frac{1}{n_1} +   \frac{1}{n_2} }$<br><br>$s_p = \sqrt{\frac{(n_1 - 1 )s_1 ^2 + (n_2 - 1 )s_2 ^2}{n_1 + n_2 - 2}}$ | $SE(\bar d) = \frac{s_d}{\sqrt{n}}$ |
| P-value | $$ \begin{array}{ll}   i.   & \Pr(T_{df} \leq t) \\   ii.  & \Pr(T_{df} \geq t) \\   iii. & 2 \Pr(T_{df} \geq |t|) \end{array} $$ | $$ \begin{array}{ll}   i.   & \Pr(T_{df} \leq t) \\   ii.  & \Pr(T_{df} \geq t) \\   iii. & 2 \Pr(T_{df} \geq |t|) \end{array} $$ | $$ \begin{array}{ll}   i.   & \Pr(T_{df} \leq t) \\   ii.  & \Pr(T_{df} \geq t) \\   iii. & 2 \Pr(T_{df} \geq |t|) \end{array}  $$ |

</font>

<br>
In the past weeks we focused on tests for:

- a __quantitative__ response variable (weeks 16 and 18);
- a __quantitative__ response variable and a __binary categorical__ explanatory variable (week 17).

This week we will investigate tests for:

- a __categorical__ response variable (*chi-square goodness-of-fit test*);
- a __categorical__ response variable and a __categorical__ explanatory variable (*chi-square test of independence*).

For the first time, you will be working with a statistical test that applies to categorical variables with more than two categories.

The two tests that you will be applying today are the chi-square goodness-of-fit test and the chi-square test of independence.
The first is used to assess whether sample results conform with an hypothesis about the proportional breakdown of the various categories in the population.
The latter tests if two categorical variables are associated in the population or not.




## Walkthrough: Birthdays of the week <br>The chi-square goodness-of-fit test {-}

- What day of the week were you born on?
- Are people equally likely to be born on any of the seven days of the week?
- Or are some days more likely to be a person’s birthday than other days?

To investigate this question, days of birth were recorded for the 147 "noted writers of the present" listed in _The World Almanac and Book of Facts 2000_. 


```{r echo=FALSE, include=FALSE}
# Generate data
library(tidyverse)

days = list(
  name = c('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'),
  times = c(17, 26, 22, 23, 19, 15, 25)
)

writers = tibble(DayOfWeek = rep(days$name, days$times))
writers = sample_n(writers, nrow(writers))

# writers = writers %>%
#   mutate(WriterID = 1:nrow(writers)) %>%
#   select(WriterID, DayOfWeek)

writers

write_tsv(writers, 'data/writers.txt', col_names = TRUE)
```

The data are stored in the file [`writers.txt`](https://edin.ac/2Ul2yLj), accessible via the link `https://edin.ac/2Ul2yLj`.



---

`r msmbstyle::question_begin(header = "&#x25BA; Question A.1")`
Identify the observational units and variable here. 

Is the variable categorical or quantitative? If it is categorical, is it also binary? 

- *Observational units:* 

- *Variable:* <br>*Type:*
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
- *Observational units:* The 147 noted writers of the present listed in _World Almanac and Book of Facts 2000_

- *Variable:* Day of the week on which the birthday falls <br>*Type:* Categorical
`r msmbstyle::solution_end()`



---




`r msmbstyle::question_begin(header = "&#x25BA; Question A.2")`
Read the data into R.
*[Hint: The data is a __.txt__ file - think about what function you are going to use.]*

Summarise the data by creating a frequency table of the seven days of the week.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
Let's read the data into R:
```{r}
library(tidyverse)

writers <- read_tsv('https://edin.ac/2Ul2yLj', col_names = TRUE)
```

Inspect the first six rows:
```{r}
head(writers)
```

Check the dimensions of the tibble:
```{r}
dim(writers)
```

Day of the week is encoded as a character rather than a factor (remember that day of the week is categorical). Let's fix the encoding:
```{r}
writers <- writers %>%
  mutate(DayOfWeek = factor(DayOfWeek))

head(writers)
```

Check that the levels of the factor are in the order we expect:
```{r}
levels(writers$DayOfWeek)
```

They are not, so let's fix this:
```{r}
writers <- writers %>%
  mutate(
    DayOfWeek = factor(DayOfWeek, 
                       levels= c("Monday", "Tuesday", "Wednesday", 
                                 "Thursday", "Friday", "Saturday", "Sunday"))
  )

levels(writers$DayOfWeek)
```

There are many equivalent ways to create a table of counts. One option could be:
```{r}
writers_table <- writers %>% 
  group_by(DayOfWeek) %>% 
  summarise(count = n())

writers_table
```

`r msmbstyle::solution_end()`



---



`r msmbstyle::question_begin(header = "&#x25BA; Question A.3")`
Construct a bar graph of these data which displays proportions on the y-axis.

Comment on what it reveals about whether the seven days of the week are equally likely to be a person’s birthday.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
The proportion of birthdays on a Monday is calculated as the count of birthdays on a Monday divided by the sample size, $n =$ `r nrow(writers)`. The procedure is similar for the other days of the week.
We can obtain the sample size using the function `nrow()`, which returns the total number of rows in the tibble:
```{r}
nrow(writers)
```

First, we compute the sample proportions:
```{r}
writers_table <- writers_table %>%
  mutate(prop = count / nrow(writers))

writers_table
```


_Note: Remember that the sample proportions sum to one:_
```{r}
sum(writers_table$prop)
```


The following bar graph displays the data:
```{r}
ggplot(writers_table, aes(x = DayOfWeek, y = prop)) +
  geom_col(fill = 'lightblue') +
  labs(x = 'Day of the week', y = 'Proportion of writers') +
  theme_classic(base_size = 15)
```

There does not appear to be a great difference in the proportion of people born on any given day of the week.
`r msmbstyle::solution_end()`


---

`r msmbstyle::question_begin(header = "&#x25BA; Question A.4")`
Let $p_{Mo}$ represent the proportion of **all** people who were born on a Monday, or equivalently, the probability that a randomly selected individual was born on a Monday.

Similarly, define $p_{Tu},\ p_{We},\ ...,\ p_{Su}$.

Are these parameters or statistics? Explain why.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
The values $p_{Tu},\ p_{We},\ ...,\ p_{Su}$  represent the proportion of **all** people who were born on Tuesday, Wednesday, ..., and Sunday, respectively.

These values are parameters because they describe the entire population.
`r msmbstyle::solution_end()`



---


`r msmbstyle::question_begin(header = "&#x25BA; Question A.5")`
The null hypothesis says that the seven days of the week are _equally likely_ to be a person’s birthday. 

In that case, what are the values of $p_{Mo},\ p_{Tu},\ ...,\ p_{Su}$?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
The values of the proportions specified by the null hypothesis are:
<center>
$$
\begin{array}{l}
p_{Mo} = 1/7 = .1429 \\
p_{Tu} = 1/7 = .1429 \\
... \\
p_{Su} = 1/7 = .1429
\end{array}
$$
</center>
`r msmbstyle::solution_end()`

---

<div class="def">
<br>
The test procedure we are about to apply is called a __chi-square goodness-of-fit test__. 

It applies to a categorical variable, which does not have to be binary. 
The null hypothesis asserts specific values for the population proportion in each category. 
The alternative hypothesis simply states that **at least one** of the population proportions is not as specified in the null hypothesis. 

As always, the test statistic measures how far the observed sample results deviate from what is expected if the null hypothesis is true. 
With a chi-square test, you construct the test statistic by comparing the _observed_ sample counts in each category to the _expected_ counts under the null hypothesis.
</div>

---


`r msmbstyle::question_begin(header = "&#x25BA; Question A.6")`
Intuitively, what value would make sense for the expected count of Monday birthdays in this study (with a sample size of 147), under the null hypothesis that one-seventh of all birthdays occur on Mondays? Explain why.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
You would expect the frequency of birthdays on a Monday to be $147 \times \frac{1}{7}= 21$.

We would multiply the total sample size by the proportion of birthdays we expect on a Monday.
`r msmbstyle::solution_end()`


---

<div class="def">
<br>
We calculate the __expected count__ for a particular category by multiplying the sample size by the hypothesized population proportion for that category:
<center>
$$
E_i = n \times p_{i0}
$$
</center>

*[Note: the subscript $i$ denotes the particular category, while the subscript $0$ on $p_{i0}$ emphasizes that this is the hypothesized value for $p_i$ in the null hypothesis, $H_0$.]*

</div>

---


`r msmbstyle::question_begin(header = "&#x25BA; Question A.7")`
Calculate the expected counts for each of the seven days. 
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
As in this particular study $p_{Mo0} = p_{Tu0} = \cdots = p_{Su0} = 1/7$, the expected counts will all be computed as $147 (1/7) = 21$.

The sample size is:
```{r}
n <- nrow(writers)
n
```

We multiply the sample size by the hypothesized proportion for each category, 1/7:
```{r}
writers_table <- 
  writers %>%
  group_by(DayOfWeek) %>%
  summarise(observed = n()) %>%
  mutate(expected = rep(n * (1/7), 7))

writers_table
```

_Note: The command `rep(x, times)` will repeat `x` a given number of `times`. For instance:_
```{r}
rep("a", 4)
```

_We calculated the expected count, `n * (1/7)`, and then repeated it for the 7 days of the week, as in this particular example the null hypothesis specifies the same proportions for all seven days of the week:_
```{r}
rep(n * (1/7), 7)
```
`r msmbstyle::solution_end()`


---

<div class="def">
<br>
How do we measure how far are the observed counts from the expected counts under the null hypothesis?

If you simply subtract the expected counts from the observed counts and then add them up, you will obtain zero:

```{r}
writers_diff <- writers_table %>%
  mutate(diff = observed - expected)

writers_diff

# Check that the sum of (observed - expected) is zero
sum(writers_diff$diff)
```

Instead, we will __square the differences__ between the observed and expected counts, and then add them up.

One issue, however, remains to be solved. 
A squared difference between observed and expected counts of 100 has a different weight in these two scenarios:

__Scenario 1.__ $O = 30$ and $E = 20$ lead to a squared difference $(O - E)^2 = 10^2 = 100$.

__Scenario 2.__ $O = 3000$ and $E = 2990$ lead to a squared difference $(O - E)^2 = 10^2 = 100$.

However, it is clear that a squared difference of 100 in Scenario 1 is much more substantial than a squared difference of 100 in Scenario 2. 
It is for this reason that we divide the squared differences by the the expected counts to **"standardize" the squared deviation**.

The test-statistic (denoted $\chi^2$, spelled _chi-square_, pronounced "kai-square") is obtained by adding up the standardized squared deviations:
<center>
$$
\chi^2 = \sum_{i} \frac{(O_i - E_i)^2}{E_i}
$$
</center>
and the sum is over all categories.

**Note:** As $(O - E)^2$ is equal to $(E - O)^2$, you can equivalently write the test-statistic as:
<center>
$$
\chi^2 = \sum_{i} \frac{(O_i - E_i)^2}{E_i} = \sum_{i} \frac{(E_i - O_i)^2}{E_i}.
$$
</center>
</div>

---


`r msmbstyle::question_begin(header = "&#x25BA; Question A.8")`
For each of the seven days of the week, compute $\frac{(O - E)^2}{E}$. 
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
We can compute the standardized squared deviations as follows:
```{r}
writers_table <- writers_table %>%
  mutate(std_sq_diff = (observed - expected)^2 / expected)

writers_table
```

`r msmbstyle::solution_end()`

---

`r msmbstyle::question_begin(header = "&#x25BA; Question A.9")`
Add the standardized squared differences up, in order to compute the chi-square test statistic.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
The value of the chi-square test statistic in this sample is:

```{r}
chi_stat <- sum(writers_table$std_sq_diff)
chi_stat
```

Equivalently, as `$` is just a shortcut for the `pull()` function,
```{r eval=FALSE}
chi_stat <- writers_table %>%
  pull(std_sq_diff) %>%
  sum()
```
`r msmbstyle::solution_end()`


---


`r msmbstyle::question_begin(header = "&#x25BA; Question A.10")`
What would be the value of the chi-square statistic if the observed and expected frequencies had been exactly equal?

Based on this, what kind of values (e.g., large or small) of the test statistic constitute evidence against the null hypothesis that the seven days of the week are equally likely to be a person’s birthday?

Do you think the value we just calculated (`r chi_stat %>% round(3)`) provides convincing evidence? If you are not sure, what additional information do you need? Explain your reasoning.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
If the observed and expected frequencies had been exactly equal, the chi-square statistic would have been 0 and you would have had no reason to doubt the null hypothesis that all days of the week are equally likely to be someone's birthday.
<center>
$$
\chi^2 = \frac{(21 - 21)^2}{21} + \cdots + \frac{(21 - 21)^2}{21} = 0
$$
</center>


Large values of the test statistic constitute evidence against the null hypothesis. 
Answers will vary about whether this test statistic provides convincing evidence. 
The additional evidence you need is the p-value. 
You need to know how likely it is that you would obtain a test statistic this large (or larger) by random chance alone if all seven days of the week are equally likely to be a person’s birthday.

`r msmbstyle::solution_end()`


---

<div class="def">
<br>
As always, your next step is to calculate the p-value. 

The p-value tells you the probability of getting sample data at least as far from the hypothesized proportions as these data are, by random chance if the null hypothesis is true. 

So again, a small p-value indicates the sample data is unlikely to have occurred by chance alone if the null hypothesis is true, providing evidence in favour of the alternative. 

When the test statistic is large enough to produce a small p-value, then the sample data provide strong evidence against the null hypothesis
</div>

---

`r msmbstyle::question_begin(header = "&#x25BA; Question A.11")`
Calculate the p-value for this test using the function `pchisq(<test statistic>, df)`.

Write a sentence interpreting the p-value meaning in the context of this study about birthdays of the week.

*[Hint: the degrees of freedom are equal to the number of categories minus 1.]*
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
We want the probability to the right of the chi-square test statistic:
```{r}
1 - pchisq(chi_stat, df = 6)
```

If all seven days of the week are equally likely to be a person’s birthday, the probability that you would obtain a test statistic this large (or larger) by random chance alone is .56.
`r msmbstyle::solution_end()`

---


`r msmbstyle::question_begin(header = "&#x25BA; Question A.12")`
Based on this p-value, what would be your test decision at the $\alpha = .10$ level?

And at the $\alpha = .05$ and $\alpha = .01$ levels?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`

At the $\alpha = .10$ level, we fail to reject the null hypothesis that all seven days of the week are equally likely to be a person's birthday, as the p-value is larger than the significance level.

The same conclusion would be reached at the other two significance levels.

`r msmbstyle::solution_end()`


---

<div class="def">
#### Validity conditions {-}
Certain technical conditions must be satisfied for this chi-square procedure to provide accurate p-values. 

In addition to requiring a **random sample** from the population of interest, **all expected counts need to be at least five**. 

When this condition is not met, one option is to combine similar categories together to force all expected counts to be at least five. 
</div>

---

`r msmbstyle::question_begin(header = "&#x25BA; Question A.13")`
Is the expected counts condition satisfied for this study on birthdays? 

What about the random sampling condition? 

If not, would you be comfortable in generalising the results to a larger population anyway? Explain why. 
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
Yes, all the expected counts are 21, which is greater than 5. 

However, the random sampling condition is not met. 

These are the birthdays of "noted writers of the present" which is not a random sample of the population of all citizens. 
`r msmbstyle::solution_end()`


---

`r msmbstyle::question_begin(header = "&#x25BA; Question A.14")`
Summarise your conclusion about whether these sample data provide evidence against the null hypothesis that any of the seven days of the week are equally likely to be a person’s birthday.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`
You have no statistical evidence against the null hypothesis that the seven days of the week are all equally likely to be a person’s birthday (at least for the population of famous writers).

We can't generalise this to the population of all citizens for the shortcoming discussed in the previous question.
`r msmbstyle::solution_end()`


---

<div class="lo">
#### Technology detour: The chi-square test using built-in R functions {-}

In R, we can perform a chi-square test using the function 

`chisq.test(<frequency table>, p=<null hypothesis>)`


__Step 1.__ Frequency table and null hypothesis:
```{r}
observed <- writers %>% 
  group_by(DayOfWeek) %>% 
  summarise(observed = n()) %>%
  pull(observed)

observed

probabilities <- rep(1/7, 7)
probabilities
```

__Step 2.__ Use the function `chisq.test`:
```{r}
gof_test <- chisq.test(observed, p = probabilities)
gof_test
```

The results match those that we calculated above.

The object returned by the function `chisq.test()` contains different variables:
```{r}
names(gof_test)
```

You can obtain the expected counts as:
```{r}
gof_test$expected
```
</div>


---

`r msmbstyle::question_begin(header = "&#x25BA; Question A.15")`
Which days of the week had the highest contributions to the chi-square test statistic?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(toggle = TOGGLE)`

**Standardized squared deviations**

One possible way to answer this question is to look at the individual contribution of each category to the chi-square statistic. You computed these values in Question A.8.

The categories Tuesday and Saturday contributed the most to the chi-square statistic as they had the highest standardized squared deviations, equal to 1.19 and 1.71, respectively.
From the barplot in Question A.3 we notice that Tuesday had a higher proportion than expected under the null, while Saturday a lower proportion than expected under the null.
```{r}
ggplot(writers_table, aes(x = DayOfWeek, y = observed / nrow(writers))) +
  geom_col(fill = 'lightblue') +
  geom_hline(yintercept = 1/7, color = 'red') +
  labs(x = 'Day of the week', y = 'Proportion of writers') +
  theme_classic(base_size = 15)
```

**Pearson residuals**

Equivalently, you could answer by looking at Pearson residuals:
```{r}
gof_test$residuals
```

The highest values are for Tuesday and Saturday. The value for Tuesday, 1.09, is positive. This means that the observed counts were larger than expected.
In the same way we interpret the residual for Saturday, -1.31. As this value is negative, this means that the observed counts of birthdays on a Saturday were less than expected.
`r msmbstyle::solution_end()`



## Lab: Joking for a tip <br>The chi-square test of independence {-}

Can telling a joke affect whether or not a waiter in a coffee bar receives a tip from a customer?

A [study](https://doi.org/10.1111/j.1559-1816.2002.tb00266.x) published in the Journal of Applied Social Psychology^[Gueaguen, N. (2002). The Effects of a Joke on Tipping When It Is Delivered at the Same Time as the Bill. _Journal of Applied Social Psychology, 32_(9), 1955-1963.] investigated this question at a coffee bar of a famous seaside resort on the west Atlantic coast of France.
The waiter randomly assigned coffee-ordering customers to one of three groups.
When receiving the bill, one group also received a card telling a joke, another group received a card containing an advertisement for a local restaurant, and a third group received no card at all. 

The data are stored in the file [`TipJoke.csv`](https://edin.ac/2U6zfgO), accessible via the link `https://edin.ac/2U6zfgO`. 
The variables are:

- `Card`: None, Joke, Ad.
- `Tip`: 1 = The customer left a tip, 0 = The customer did not leave tip. 

In the following, we will consider leaving a tip as "success".

---

`r msmbstyle::question_begin(header = "&#x25BA; Question B.1")`
Load the data into R and inspect it. *[Hint: the data is a __.csv__ file - think about what function you are going to use.]*

Pay particular attention to:

- the variable names;
- the dimensions of the tibble;
- the format of the data (i.e., make sure that variables are correctly encoded).
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden = HIDDEN_SOLS, toggle = TOGGLE)`
Load the data:
```{r}
library(tidyverse)

tipjoke <- read_csv('https://edin.ac/2U6zfgO', col_names = TRUE)
```

Inspect the first six rows:
```{r}
head(tipjoke)
```

Check the dimensions of the tibble:
```{r}
dim(tipjoke)
```
We have 211 observations on 2 variables.

Make the variables factors and give more meaningful names to the variable `Tip`:
```{r}
tipjoke <- tipjoke %>%
  mutate(Card = factor(Card),
         Tip = factor(Tip, levels = c(0, 1), labels = c('NoTip', 'Tip')))

head(tipjoke)
```

`r msmbstyle::solution_end()`

---

`r msmbstyle::question_begin(header = "&#x25BA; Question B.2")`
Identify the observational units in this study.

Identify the explanatory and response variables in this study, and classify them as either categorical (also binary) or quantitative.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden = HIDDEN_SOLS, toggle = TOGGLE)`
- *Observational units:* The 211 adults ordering coffee at the bar of the seaside resort on the west coast of France.

- *Explanatory variable:* Type of card (if any) given to the customer. <br>*Type:* Categorical but not binary as it has three categories.

- *Response variable:* Whether or not the customer left a tip. <br>*Type:* Categorical and binary.
`r msmbstyle::solution_end()`

---


`r msmbstyle::question_begin(header = "&#x25BA; Question B.3")`
Create a barplot that visually summarises the data.

Comment on what the graph reveals.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden = HIDDEN_SOLS, toggle = TOGGLE)`
```{r}
ggplot(tipjoke, aes(x = Card, fill = Tip)) +
  geom_bar()
```

From the stacked bar graph, it appears that perhaps the joke card does produce a higher success rate, but we need to investigate whether the differences are statistically significant.
`r msmbstyle::solution_end()`


---


`r msmbstyle::question_begin(header = "&#x25BA; Question B.4")`
Create a two-way table showing how many customers left or not a tip for each type of card (including none given).
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden = HIDDEN_SOLS, toggle = TOGGLE)`
In R, we can construct a two-way table using the function `table()`:
```{r}
observed <- table(tipjoke$Tip, tipjoke$Card)
observed
```
`r msmbstyle::solution_end()`


---

`r msmbstyle::question_begin(header = "&#x25BA; Question B.5")`
State, in words, the null and alternative hypotheses of this study.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden = HIDDEN_SOLS, toggle = TOGGLE)`
- The null hypothesis asserts that whether a customer left a tip or not is independent of the type of card given.
- The alternative hypothesis says that the two variables are associated (or related).
`r msmbstyle::solution_end()`

---


`r msmbstyle::question_begin(header = "&#x25BA; Question B.6")`
Which test would you use to test the hypotheses stated in the previous question?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden = HIDDEN_SOLS, toggle = TOGGLE)`
We need to use a chi-square test of independence.
`r msmbstyle::solution_end()`


---

`r msmbstyle::question_begin(header = "&#x25BA; Question B.7")`
Compute the value of the test statistic and corresponding p-value.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden = HIDDEN_SOLS, toggle = TOGGLE)`
```{r}
ind_test <- chisq.test(observed)
ind_test
```

The chi-square statistic in the sample is $\chi^2 = 9.953$.

The p-value is $\Pr(\chi^2(2) \geq 9.953) = 0.007$.

`r msmbstyle::solution_end()`


---

`r msmbstyle::question_begin(header = "&#x25BA; Question B.8")`
Inspect the expected frequencies.

Look at the Pearson residuals and comment on what you notice.

*[Hint: If `out <- chisq.test(<table>)`, the expected counts can be obtained as `out$expected` and the Pearson residuals as `out$residuals`.]*
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden = HIDDEN_SOLS, toggle = TOGGLE)`
The output of `chisq.test` returns many objects:
```{r}
names(ind_test)
```

We are interested in the `expected` frequencies:
```{r}
ind_test$expected
```

Recall the observed frequencies:
```{r}
observed
```

More customers than expected under the null hypothesis:

- didn't tip after receiving an advertising card;
- tipped after receiving a joke card;
- didn't tip after receiving no card.

Less customers than expected under the null hypothesis:

- tipped after receiving an advertising card;
- didn't tip after receiving a joke card;
- tipped after receiving no card.


The Pearson residuals are:
```{r}
ind_test$residuals
```

The top three contributions to the chi-square statistic are due to:

- more customers than expected under the null hypothesis tipped after receiving a joke card;
- less customers than expected under the null hypothesis tipped after receiving an advertisement card;
- less customers than expected under the null hypothesis didn't tip after receiving a joke card.

`r msmbstyle::solution_end()`


---

`r msmbstyle::question_begin(header = "&#x25BA; Question B.9")`
Check that the assumptions of the test are satisfied. These are the conditions required for the test results to be valid.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden = HIDDEN_SOLS, toggle = TOGGLE)`
Assumptions checks:

- A simple random sample is taken from a large population.
- Each outcome can be classified into one cell according to its category on one variable (type of card) and its category on the second variable (whether a tip was given or not).
- The expected frequency in each cell is 5 or greater.

Hence, the conditions required for the test results to be valid are met.

`r msmbstyle::solution_end()`

---

`r msmbstyle::question_begin(header = "&#x25BA; Question B.10")`
Write your conclusion linked to the test results and in the context of the problem.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden = HIDDEN_SOLS, toggle = TOGGLE)`
A chi-square test of independence between tipping and card type was performed ($\chi^2(2) = 9.953$, $p = .007$). 
The chi-square test shows that the tip rate differs depending on the which card (Joke, Ad, or None) the waiter gave to the customer. In other words, the two variables are associated.
To understand the nature of this difference in the chi-square test, we need to compare observed and expected frequencies. 
The observed number of tips in the joke group (30) is quite a bit higher than expected (20.47), while the observed counts for both the advertising and no card groups are lower than expected. 

So, if you ever find yourself taking coffee orders in a cafe of a seaside resort in the west coast of France, you probably want to learn some jokes!
`r msmbstyle::solution_end()`


## Summary {-}

In this lab you applied two types of statistical tests which apply to categorical variables having two or more categories.

The __chi-square goodness-of-fit__ test involves **one categorical variable**. It determines whether it is reasonable to assume that your sample came from a population in which, for each category, the proportion of the population that falls into the category is equal to some hypothesized proportion. 

The __chi-square test of independence__ involves **two categorical variables**. It determines whether it is reasonable to assume that two categorical variables are independent or not in the population. If they are not independent, we also say that the two variables are __associated__.

Both tests involve using the chi-square test statistic, which has the following form:
<center>
$$
\chi^2 = \sum_{all\ cells} \frac{(Observed - Expected)^2}{Expected}
$$
</center>

The degrees of freedom are:

- *Goodness-of-fit:* df = # categories - 1;

- *Independence:* df = (# rows - 1) $\times$ (# columns - 1).


**Validity conditions.** For the test results to be valid, the data should come from a random sample of the population of interest, and the expected counts should all be at least 5.



## Glossary {-}

- _Cell contribution._ The contribution of a cell in a table to the chi-square statistic. It is helpful in determining where large differences between observed data and what would be expected if the null hypothesis were true, exist.
- _Chi-square statistic._ A standardized statistic for quantifying the distance between the  observed frequencies in the sample with those expected under the null hypothesis.
- _Expected counts._ The expected frequencies for the cells of the table assuming the null hypothesis is true.
- _Chi-square goodness-of-fit test._ Are the proportions of the different categories in the population equal to the hypothesized proportions?
- _Chi-square test of independence._ Are two categorical variables independent in the population?
- _Observed counts._ The observed frequencies in the cells of the table from the study.
- _Validity conditions for the chi-square test._ All expected counts must be at least 5.


## References {-}

- Gueaguen, N. (2002). The Effects of a Joke on Tipping When It Is Delivered at the Same Time as the Bill. _Journal of Applied Social Psychology, 32_(9), 1955-1963.

Material adapted from:

- Rossman, A. J., & Chance, B. L. (2011). _Workshop statistics: discovery with data._ John Wiley & Sons.
- Cannon, A. R., Cobb, G. W., Hartlaub, B. A., Legler, J. M., Lock, R. H., Moore, T. L., ... & Witmer, J. (2013). _Stat2: Building models for a world of data._ W.H. Freeman.
- Tintle, N., Chance, B. L., Cobb, G. W., Rossman, A. J., Roy, S., Swanson, T., & VanderStoep, J. (2015). _Introduction to statistical investigations._ New York: Wiley.
