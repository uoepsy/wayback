<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lecture 10: Samples, Statistics &amp; Sampling Distributions </title>
    <meta charset="utf-8" />
    <meta name="author" content="TOM BOOTH &amp; ALEX DOUMAS" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <b>Lecture 10: Samples, Statistics &amp; Sampling Distributions </b>
## Data Analysis for Psychology in R 1<br><br>
### TOM BOOTH &amp; ALEX DOUMAS
### Department of Psychology<br>The University of Edinburgh
### AY 2020-2021

---








# Week's Learning Objectives
1. Understand the difference between a population parameter and a sample statistic.
2. Understand the concept and construction of sampling distributions. 
3. Understand the effect of sample size on the sampling distribution.
4. Understand how to quantify the variability of a sample statistic and sampling distribution (standard error). 

---
# Topics for today
- Understand the concept principles sampling from populations.
- Be familiar with the specific statistical terminology for sampling.
- Understand the concept of a sampling distribution.

---
## Concepts to carry forward
- Data can be of different types.
- Dependent on type (continuous vs. categorical), we can visualise and describe the distribution of data differently.
- When thinking about events ("things happening") we can assign probabilities to the event.
- We can define a probability distribution that describes the probability of all possible events.

---
## In Psych Stats
- In psychology, we design a study, to calculate a value that carries some meaning.
    - Reaction time of one group vs another.
- Given it has meaning based on the study design, we want to know something about the number:
    - Is it unusual or not?
- This is the task for the next 4-5 weeks. 

---
## Today
- We will talk about populations, samples, and sampling.
- Basic concepts of sampling may seem simple and intuitive.
- These concepts will be very useful when we start talking about statistical inference.
    - Statistical inference = how we make decisions about data.

---
## A question
- Suppose I wanted to know the proportion of UG students at the University of Edinburgh were born in Scotland?
    - In stats talk, all UG at the UoE are our **population**.
    - The proportion of students born in Scotland is the **population parameter** (the thing we are interested in).
- What is the best way to find this out exactly?
- What else might we do?

---
## What is a sample? 
- A sample is a portion of the population that you check.
- Use the sample as an estimate of the real population

---
## Parameters and point-estimates
- Key idea: 
    - There is population parameter (proportion of Scottish born students at UoE) we are interested in. This is a *true* value of the world.
    - We can draw a sample, and calculate this proportion (statistic) in the sample.
      - In a single sample, this **point-estimate** is our best guess at the population parameter.
    - We might use something like our class as a sample to estimate the true value in the University.

---
## 2017/18 actual proportion

&lt;table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Scottish &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Freq &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; No &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 13926 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.54 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Yes &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 12025 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.46 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

- Let's assume this is the true value now (it was in 17/18).
- As we've just said, we can draw samples from the population and use it to estimate the population value.
- A **sampling distribution** is a probability distribution of some statistic obtained from sampling the population. 
- If we draw a bunch of samples from the population, each is an estimate of the real population value. 
  - Let's simulate drawing a bunch of samples of students from the University and see what proportion of each sample is born in Scotland.
  - We'll draw 10 samples of 10 students from the population.
  - We then create a histogram showing how frequently our samples showed particular proportions of students born in Scotland...

---
## Visualizing sampling distributions
![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;



---
## Visualizing sampling distributions
![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---
## Visualizing sampling distributions
![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

---
## Sampling distributions (2)
- We have just created three sampling distributions.
- Each of these look different.
- Each sampling distribution is characterising the **sampling variability** in our **estimate** of the **parameter** of interest (proportion of Scottish students at UoE).
- What do we notice? 
  - Do samples with values close to the population value tend to be more or less likely? 

---
## More samples
- So far we have taken 10 samples.
- What if we took more?

---
## More samples

![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

---
## More samples

![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

---
## More samples

![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

- What do you notice about the plots on the last three slides?

---
## Frequency = probability
- At this point lets pause and remember some things from probability.
- When we spoke about probability, we spoke about the relation to frequency.
- If something did not happen very often, it has a lower probability.
    - Now think about our sampling distributions of the proportion of Scottish students.

---
## Frequency gives us probability
.pull-left[
![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;
]

.pull-right[
![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;
]

---
## Bigger samples

![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

- So above is a frequency distribution for `\(n\)`=10.
- Let's see what happens when we make `\(n\)` bigger.

---
## Bigger samples

![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

---
## Bigger samples

![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;

- What do you notice about the last three slides?

---
## Properties of sampling distributions
- Remember: frequency distributions are characterising the variability in sample estimates.
- Variability can be thought of as the spread in data/plots.
- So as we increase `\(n\)` we are getting less variable samples (harder to get an unrepresentative sample as your `\(n\)` increases).

---
## Properties of sampling distributions
- Let's put this phenomenon in the language of probability: as `\(n\)` increases, the probability of observing an estimate in a sample that is a long way from the population parameter (here 0.46) decreases (becomes less probable).
- So when we have large samples, our estimates from those samples are likely to be closer to the population value.
    - That's good! 

---
## Standard error
- We can formally calculate the "narrowness" of a sampling distribution. 
    - This is essentially calculating the standard deviation (as we have done before) of the sampling distribution.
    - Or at least approximating it!
- In the context of sampling distributions, this is called the **standard error**

---
## Mean &amp; SE of sampling distribution
- Mean of the sampling distribution is close to the population parameter.
    - Even with a small number of samples.
- As the number of samples increases:
  - The mean of the sampling distribution approaches the population mean.
  - The sampling distribution approaches a normal distribution.
    - Point-estimates pile up around the population value.
- As the n per sample increases, the SE of the sampling distribution decreases (becomes narrower).
    - With large n, all our point-estimates are closer to the population parameter.

---
## Central Limit Theorem
- What we have seen throughout this lecture is a demonstration of an important concept in statistics - namely, **central limit theorem**.  
- The central limit theorem (roughly) states that when estimates of sample means are based on increasingly large samples ($n$), the sampling distribution of means becomes more normal (symmetric), and narrower (quantified by the standard error).

---
## Central limit theorem
- We have briefly noted CLT before. To refresh;
- The Central Limit Theorem states that the sampling distribution of the sample means from any underlying distribution with a defined mean and variance, approaches a normal distribution as the sample size gets larger.
- The resultant sampling distribution has:
  - `\(\bar{x} = \mu\)`
  - `\(\sigma_{\bar{x}}^{2} = \frac{\sigma^2}{N}\)`
  - `\(\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{N}}\)`

---
## Uniform distribution
- Continuous probability distribution.
- There is an equal probability for all values within a given range.
- Parameters:
  - `\(a(min)\)` and `\(b(max)\)`

$$
Mean = \frac{1}{2}(a+b)
$$

- And

$$
Variance = \frac{1}{12}(b-a)^2
$$

---
## Uniform distribution

![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

---
## Uniform distribution


.pull-left[
&lt;img src="dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-15-1.png" width="60%" /&gt;&lt;img src="dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-15-2.png" width="60%" /&gt;
]

.pull-right[
&lt;img src="dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-16-1.png" width="60%" /&gt;&lt;img src="dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-16-2.png" width="60%" /&gt;
]

---
## Chi-square distribution
- Continuous probability distribution
- Non-symmetric
- Parameters = degrees of freedom

$$
Mean = df
$$

- and

$$
Variance = 2*df
$$

---
## Chi-square distribution

![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;

---
## Chi-square distribution

.pull-left[
&lt;img src="dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-19-1.png" width="60%" /&gt;&lt;img src="dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-19-2.png" width="60%" /&gt;
]

.pull-right[
&lt;img src="dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-20-1.png" width="60%" /&gt;&lt;img src="dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-20-2.png" width="60%" /&gt;
]

---
## t-distribution
- Continuous probability distribution.
- Symmetric and uni-modal (similar to the normal distribution).
  - "Heavier tails" = greater chance of observing a value further from the mean
- Parameters:
  - `\(\nu = n-1\)`
  
$$
Mean = 0, \nu&gt;1
$$

- and

$$
Variance = \frac{\nu}{\nu - 2}, \nu &gt; 2
$$

---
## t-distribution

![](dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;

---
## t-distribution


.pull-left[
&lt;img src="dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-23-1.png" width="60%" /&gt;&lt;img src="dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-23-2.png" width="60%" /&gt;
]

.pull-right[
&lt;img src="dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-24-1.png" width="60%" /&gt;&lt;img src="dapR1_lec10_Samples-SamplingDist_files/figure-html/unnamed-chunk-24-2.png" width="60%" /&gt;
]

---
## Sampling distributions
- `\(\chi^2\)` distribution, *t*-distribution and binomial distribution are all commonly used for statistical inference.
- What the CLT demonstrations above show, is that we can often use the normal distribution as an approximation of the sampling distribution. 

---
## Standard error 
- One of the big points we have emphasized is sampling variability is characterized by the SD of the sampling distribution.
- But how do we obtain this from a single sample?
- We have already seen the answer thanks to CLT.
- In the limit, the sampling distribution has:
  - `\(\bar{x} = \mu\)`
  - `\(\sigma_{\bar{x}}^{2} = \frac{\sigma^2}{N}\)`
  - `\(\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{N}}\)` = standard error

---
## Key terminology
- **Census**: process of asking every member of a population.
- **Sampling**: process of selecting subsets of populations.
- **Population**: the complete set of units of interest.
- **Sample**: A subset of the population

---
## Key terminology
- **Parameter**: value of of interest in the population.
- **Point estimate**: our "best guess" at the parameter of interest from a sample.
- **Sampling distribution**: the distribution of estimates of the population parameter.
- **Standard error**: quantification of the variation in estimates.

---
## Features of samples
- Is our sample...
    - Biased?
    - Representative?
    - Random?

---
## Good samples
- If a sample of `\(n\)` is drawn at random, it will be unbiased and representative of `\(N\)`
- Our point estimates from such samples will be good guesses at the population parameter.
    - Without the need for census.

---
# Summary of today
- Samples are used to estimate the population. 
- Samples provide point estimates of population parameters. 
- Properties of samples and sampling distributions.
- Properties of good samples.

---
# Next tasks
+ Look back over any material from term.
+ This week:
  + Complete your lab
  + Come to office hours
  + Come to Q&amp;A session
  + Weekly quiz - on week 10 (lect 9) content
      + Open Tuesday 09:00
      + Closes Monday 17:00



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
