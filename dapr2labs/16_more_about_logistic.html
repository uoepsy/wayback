<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>More About Logistic Regression</title>

<script src="site_libs/header-attrs-2.8/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);
e.style.display = ((e.style.display!='none') ? 'none' : 'block');
if(f.classList.contains('fa-plus')) {
    f.classList.add('fa-minus')
    f.classList.remove('fa-plus')
} else {
    f.classList.add('fa-plus')
    f.classList.remove('fa-minus')
}
}
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<link rel="stylesheet" href="assets/style-labs.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"><strong>DAPR2</strong></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fas fa-home"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Simple Linear Regression
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="01_models.html">1/1: Functions and models</a>
    </li>
    <li>
      <a href="02_slr.html">1/2: Simple linear regression</a>
    </li>
    <li>
      <a href="03_slr_model_fit.html">1/3: Model Fit, Standardized Coefficients</a>
    </li>
    <li>
      <a href="04_slr_assumptions.html">1/4: Assumptions &amp; Diagnostics</a>
    </li>
    <li>
      <a href="05_slr_writeup.html">1/5: Writing-up</a>
    </li>
    <li class="dropdown-header">1/6: Break week</li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Multiple Linear Regression
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="06_mlr.html">1/7: Basics</a>
    </li>
    <li>
      <a href="07_mlr_int.html">1/8: Interactions</a>
    </li>
    <li>
      <a href="08_mlr_assumpt.html">1/9: Assumptions &amp; Diagnostics</a>
    </li>
    <li>
      <a href="09_mlr_select.html">1/10: Model Fit, Model Comparison, Model Selection</a>
    </li>
    <li>
      <a href="10_mlr_report.html">1/11: Writing-up</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Experimental Data
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="11_anova.html">2/1: ANOVA</a>
    </li>
    <li>
      <a href="11_anova.html">2/2: ANOVA (again)</a>
    </li>
    <li>
      <a href="12_factorial_anova.html">2/3: 2x2 ANOVA</a>
    </li>
    <li>
      <a href="13_multiplecomp.html">2/4: Multiple Comparisons</a>
    </li>
    <li class="dropdown-header">2/5: No exercises</li>
    <li class="dropdown-header">2/6: Break week</li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Advanced Topics for LM
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="14_bootstrap_reg.html">2/7: Bootstrap</a>
    </li>
    <li>
      <a href="15_binary_logistic.html">2/8: Binary Logistic Regression</a>
    </li>
    <li>
      <a href="16_more_about_logistic.html">2/9: More About Logistic Regression</a>
    </li>
    <li>
      <a href="17_power_regression.html">2/10: Sample Size and Power Analysis</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Help
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="intro_r_rstudio.html">Getting started with R &amp; RStudio</a>
    </li>
  </ul>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">More About Logistic Regression</h1>

</div>


<div class="yellow">
<p><strong>PREREQUISITES</strong></p>
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be two positive numbers:</p>
<p><span class="math display">\[
\begin{aligned}
e^{A + B} &amp;= e^A \times e^B \\
e^{\log(A)} &amp;= A \\
\log \left( \frac{A}{B} \right) &amp;= \log(A) - \log(B) \\
\log (A \times B) &amp;= \log(A) + \log(B)
\end{aligned}
\]</span></p>
</div>
<div id="senility-and-wais" class="section level1">
<h1>Senility and WAIS</h1>
<p>A sample of elderly people was given a psychiatric examination to determine symptoms of senility were present. Other measurements taken at the same time included the score on a subset of the Wechsler Adult Intelligent Scale (WAIS).</p>
<p>The data represent symptoms of senility (<code>senility</code> = 1 if symptoms are present and <code>senility</code>= 0 otherwise) and WAIS scores (<code>wais</code>) for <span class="math inline">\(n = 54\)</span> people.
You can download the data at following link: <a href="https://uoepsy.github.io/data/SenilityWAIS.csv" class="uri">https://uoepsy.github.io/data/SenilityWAIS.csv</a></p>
<pre class="r"><code>library(tidyverse)

sen &lt;- read_csv(file = &#39;https://uoepsy.github.io/data/SenilityWAIS.csv&#39;)
head(sen)</code></pre>
<pre><code>## # A tibble: 6 x 2
##    wais senility
##   &lt;dbl&gt;    &lt;dbl&gt;
## 1     9        1
## 2    13        1
## 3     6        1
## 4     8        1
## 5    10        1
## 6     4        1</code></pre>
<pre class="r"><code>dim(sen)</code></pre>
<pre><code>## [1] 54  2</code></pre>
<p>First, we should plot the data to understand what’s going on. We use <code>geom_jitter</code> rather than <code>geom_point</code> otherwise we will have multiple points on top of each other as there are multiple individuals with the same wais’ values:</p>
<pre class="r"><code>ggplot(sen, aes(wais, senility)) +
  geom_jitter(width = 0, height = 0.05) +
  labs(x = &quot;x, WAIS score&quot;, 
       y = &quot;y, Has senility symptoms?&quot;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-2-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Since we are interested in understanding how the probability of having senility symptoms changes as a function of the WAIS score, the response variable is <span class="math inline">\(y\)</span> = <code>senility</code> and the predictor (or explanatory variable) is <span class="math inline">\(x\)</span> = <code>wais</code>.</p>
<p>We will fit the following logistic regression model:
<span class="math display">\[
\log \left( \frac{p_x}{1-p_x} \right) = \beta_0 + \beta_1 \ x
\]</span>
where <span class="math inline">\(p_x = P(y = 1)\)</span> is the probability that <span class="math inline">\(y = 1\)</span> for an individual with WAIS score equal to <span class="math inline">\(x\)</span>.</p>
<pre class="r"><code>mdl1 = glm(senility ~ wais, family = binomial, data = sen)
summary(mdl1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = senility ~ wais, family = binomial, data = sen)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6702  -0.7402  -0.4749   0.5200   2.1157  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)   2.4040     1.1918   2.017  0.04369 * 
## wais         -0.3235     0.1140  -2.838  0.00453 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 61.806  on 53  degrees of freedom
## Residual deviance: 51.017  on 52  degrees of freedom
## AIC: 55.017
## 
## Number of Fisher Scoring iterations: 5</code></pre>
</div>
<div id="interpretation-of-coefficients" class="section level1">
<h1>Interpretation of coefficients</h1>
<p>To interpret the fitted coefficients, we first exponentiate the model:
<span class="math display">\[
\begin{aligned}
\log \left( \frac{p_x}{1-p_x} \right) &amp;= \beta_0 + \beta_1 x \\
e^{ \log \left( \frac{p_x}{1-p_x} \right) } &amp;= e^{\beta_0 + \beta_1 x } \\
\frac{p_x}{1-p_x} &amp;= e^{\beta_0} \ e^{\beta_1 x}
\end{aligned}
\]</span></p>
<p>and recall that the probability of success divided by the probability of failure is the odds of success. In our example, this would be the odds of showing senility symptoms.
<span class="math display">\[
\frac{p_x}{1-p_x} = \text{odds}
\]</span></p>
<p><strong>Intercept</strong></p>
<p><span class="math display">\[
\text{If }x = 0, \qquad \frac{p_0}{1-p_0} = e^{\beta_0} \ e^{\beta_1 x} = e^{\beta_0}
\]</span></p>
<p>That is, <span class="math inline">\(e^{\beta_0}\)</span> represents the odds of having symptoms of senility for individuals with a WAIS score of 0.</p>
<p>In other words, for those with a WAIS score of 0, the probability of having senility symptoms is <span class="math inline">\(e^{\beta_0}\)</span> times that of non having them.</p>
<p><strong>Slope</strong></p>
<p><span class="math display">\[
\text{If }x = 1, \qquad \frac{p_1}{1-p_1} = e^{\beta_0} \ e^{\beta_1 x} = e^{\beta_0} \ e^{\beta_1}
\]</span></p>
<p>Now consider taking the ratio of the odds of senility symptoms when <span class="math inline">\(x=1\)</span> to the odds of senility symptoms when <span class="math inline">\(x = 0\)</span>:
<span class="math display">\[
\frac{\text{odds}_{x=1}}{\text{odds}_{x=0}} = \frac{p_1 / (1 - p_1)}{p_0 / (1 - p_0)} 
= \frac{e^{\beta_0} \ e^{\beta_1}}{e^{\beta_0}}
= e^{\beta_1}
\]</span></p>
<p>So, <span class="math inline">\(e^{\beta_1}\)</span> represents the odds ratio for a 1 unit increase in WAIS score.</p>
<p>It is typically interpreted by saying that for a one-unit increase in WAIS score the odds of senility symptoms increase by a factor of <span class="math inline">\(e^{\beta_1}\)</span>.</p>
<p>Equivalently, we say that <span class="math inline">\(e^{\beta_1}\)</span> represents the multiplicative increase (or decrease) in the odds of success when <span class="math inline">\(x\)</span> is increased by 1 unit.</p>
<p><strong>In our model</strong></p>
<pre class="r"><code>exp(coef(mdl1))</code></pre>
<pre><code>## (Intercept)        wais 
##    11.06784     0.72359</code></pre>
<p>The odds of senility symptoms for individuals scoring 0 on the WAIS test is 11:1. Or, the probability of having senility symptoms for those scoring 0 on the WAIS test is 11 times the probability of not having senility symptoms.</p>
<p>For every unit increase in WAIS score, the odds of senility symptoms reduces by a factor of 0.72</p>
<p><br></p>
<div class="optional-begin">
Optional Distinct covariate values (grouped data)<span id="opt-start-209" class="fa fa-plus optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-209&#39;, &#39;opt-start-209&#39;)"></span>
</div>
<div id="opt-body-209" class="optional-body" style="display: none;">
<p>If you explore the full data, you will see that even though there are <span class="math inline">\(n = 54\)</span> individuals, there are only 17 distinct values of the covariate <span class="math inline">\(x\)</span> (representing the WAIS scores), as some people have the same WAIS scores but different <span class="math inline">\(y\)</span> values.</p>
<p>Let’s summarise the original 0/1 data by computing the total number of individual having that value of the predictor (<code>total</code>), the number of individuals with that predictor value that also showed senility symptoms (<code>count_1s</code>), and finally the number of people who did not show senility symptoms (<code>count_0s</code>). This is the “grouped data”:</p>
<pre class="r"><code>sen_grp &lt;- sen %&gt;%
  group_by(wais) %&gt;%
  summarise(total = n(),
            count_1s = sum(senility))

sen_grp</code></pre>
<pre><code>## # A tibble: 17 x 3
##     wais total count_1s
##    &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;
##  1     4     2        1
##  2     5     1        1
##  3     6     2        1
##  4     7     3        2
##  5     8     2        2
##  6     9     6        2
##  7    10     6        1
##  8    11     6        1
##  9    12     2        0
## 10    13     6        1
## 11    14     7        2
## 12    15     3        0
## 13    16     4        0
## 14    17     1        0
## 15    18     1        0
## 16    19     1        0
## 17    20     1        0</code></pre>
<p>You can fit a logistic regression model to grouped data as follows. This is <strong>completely equivalent</strong> to the one we fitted above.</p>
<ol style="list-style-type: decimal">
<li>Provide two columns, one giving the count of 1s and the other giving the count of 0s:</li>
</ol>
<pre class="r"><code>mdl1a &lt;- glm(cbind(count_1s, total - count_1s) ~ wais, data = sen_grp, family = binomial)
summary(mdl1a)</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(count_1s, total - count_1s) ~ wais, family = binomial, 
##     data = sen_grp)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9064  -0.6965  -0.2538   0.1719   1.7771  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)   2.4040     1.1918   2.017  0.04369 * 
## wais         -0.3235     0.1140  -2.838  0.00453 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 20.208  on 16  degrees of freedom
## Residual deviance:  9.419  on 15  degrees of freedom
## AIC: 27.792
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Provide the proportion of successes and the total number of people with that distinct predictor value:</li>
</ol>
<pre class="r"><code>sen_grp &lt;- sen_grp %&gt;%
  mutate(prop_1s = count_1s / total)

head(sen_grp)</code></pre>
<pre><code>## # A tibble: 6 x 4
##    wais total count_1s prop_1s
##   &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;
## 1     4     2        1   0.5  
## 2     5     1        1   1    
## 3     6     2        1   0.5  
## 4     7     3        2   0.667
## 5     8     2        2   1    
## 6     9     6        2   0.333</code></pre>
<pre class="r"><code>mdl1b &lt;- glm(prop_1s ~ wais, weights = total, data = sen_grp, family = binomial)
summary(mdl1b)</code></pre>
<pre><code>## 
## Call:
## glm(formula = prop_1s ~ wais, family = binomial, data = sen_grp, 
##     weights = total)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9064  -0.6965  -0.2538   0.1719   1.7771  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)   2.4040     1.1918   2.017  0.04369 * 
## wais         -0.3235     0.1140  -2.838  0.00453 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 20.208  on 16  degrees of freedom
## Residual deviance:  9.419  on 15  degrees of freedom
## AIC: 27.792
## 
## Number of Fisher Scoring iterations: 5</code></pre>
</div>
<p class="optional-end">
</p>
</div>
<div id="diagnosing-model-fit-residuals" class="section level1">
<h1>Diagnosing model fit: Residuals</h1>
<p>Unlike linear regression, logistic regression has 2 main types of residuals.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Pearson residuals</strong></p></li>
<li><p><strong>Deviance residuals</strong></p></li>
</ol>
<p>We will now discuss each in turn.</p>
<p>In addition, just like in linear regression, each of the above can be <strong>standardized</strong> or <strong>studentized</strong>. We won’t discuss how to standardize each of those as the formula is difficult, but the idea is to rescale the residuals to have unit variance, making them more useful to diagnose outlying observations.</p>
<p>Remember the difference?</p>
<ul>
<li><p><span class="math inline">\(i\)</span>th standardized residual = residual / SD(residual)</p></li>
<li><p><span class="math inline">\(i\)</span>th studentized residual = residual / SD(residual from model fitted without observation <span class="math inline">\(i\)</span>)</p></li>
</ul>
<div class="yellow">
<p><strong>What to look for</strong></p>
<p>We use the standardized/studentized residuals to identify outliers. If a case has a standardized/studentized residual larger than 2 in absolute value, it is deemed an outlier.</p>
<p>Some authors prefer a more conservative threshold of 3, in absolute value. That is, outliers are cases with a residual smaller than -3 or larger than 3. This is because approximately 99% of the data should be between -3 and 3 for a standardized variable.</p>
</div>
<div id="pearson-residuals" class="section level2">
<h2>Pearson residuals</h2>
<p>Pearson residuals are similar to the Pearson residuals you might remember from the chi-squared test in DAPR1, which compared Observed and Expected values, but in this case Expected means Predicted from the model:
<span class="math display">\[
Pres_i = \frac{Observed - Expected}{\sqrt{Expected}}
\]</span></p>
<p>In logistic regression, this is
<span class="math display">\[
Pres_i = \frac{y_i - \hat p_i}{\sqrt{\hat p_i (1 - \hat p_i)}}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(y_i\)</span> is the observed response for unit <span class="math inline">\(i\)</span>, either 0 (failure) or 1 (success)</li>
<li><span class="math inline">\(\hat p_i\)</span> is the model-predicted probability of success</li>
</ul>
<p>There is one residual per each case (row) in the dataset:</p>
<pre class="r"><code>Pres &lt;- residuals(mdl1, type = &#39;pearson&#39;)</code></pre>
<p>To see this, we can add a column to the data:</p>
<pre class="r"><code>sen %&gt;%
  mutate(Pres = Pres)</code></pre>
<pre><code>## # A tibble: 54 x 3
##     wais senility  Pres
##    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;
##  1     9        1 1.29 
##  2    13        1 2.46 
##  3     6        1 0.793
##  4     8        1 1.10 
##  5    10        1 1.52 
##  6     4        1 0.574
##  7    14        1 2.89 
##  8     8        1 1.10 
##  9    11        1 1.78 
## 10     7        1 0.933
## # … with 44 more rows</code></pre>
<p>The standardized Pearson residuals (having zero mean and unit standard deviation) are obtained as follows:</p>
<pre class="r"><code>SPres &lt;- rstandard(mdl1, type = &#39;pearson&#39;)</code></pre>
<p>The studentized Pearson residuals (also having zero mean and unit standard deviation) are obtained as follows:</p>
<pre class="r"><code>StuPres &lt;- rstudent(mdl1, type = &#39;pearson&#39;)</code></pre>
<p><strong>In the following, I will use the studentized residuals for the plots, but if you wish to, you can use the standardized ones.</strong></p>
<p>First, we plot the studentized Pearson Residuals against their index, and we check whether there are any values larger than 2 in absolute value (that is larger than 2 or smaller than -2). Some people prefer to be strict and deem as outlier those greater than 3 in absolute value as most values should be within -3 and 3.</p>
<pre class="r"><code>plot(StuPres, ylab = &quot;Studentized Pearson Residuals&quot;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-13-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Yes, there appear to be 3 residuals with a value slightly larger than 2 in absolute value. We will keep these in mind and check later if they are also influential points.</p>
<p><em><em>Warning:</em> Don’t inspect this plot for patterns!!! In here you might think there is a curvilinear relationship, but there isn’t.</em>
<em>The plot has this shape as the first 14 cases have a response = 1, while the remaining have a response = 0, hence the two different levels.</em></p>
<p><br></p>
<p>Plotting the (studentized) Pearson residuals against the fitted values<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> or the predictor is not very informative in logistic regression as you can see below:</p>
<pre class="r"><code>plot(fitted(mdl1), StuPres, 
     xlab = &#39;Prob. of Success&#39;, ylab = &#39;Studentized Pearson Residuals&#39;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-14-1.png" width="60%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(sen$wais, StuPres, 
     xlab = &#39;Prob. of Success&#39;, ylab = &#39;Studentized Pearson Residuals&#39;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-15-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>We only inspect residuals plot to find cases with a residual which is smaller than -2 (-3) or larger than 2 (3).</p>
<p>Sometimes a <strong>binned plot</strong><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> can be more informative, but not always! It works by combining together all responses for people having the same covariate <span class="math inline">\(x_i\)</span> value, and taking the average studentized Pearson residual for those.</p>
<p>Before using this function, make sure you have installed the <strong>arm</strong> package!</p>
<pre class="r"><code>arm::binnedplot(fitted(mdl1), StuPres, 
                xlab = &#39;Prob. of Success&#39;, ylab = &#39;Studentized Pearson Residuals&#39;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-16-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>There doesn’t appear to be any extreme residuals.</p>
</div>
<div id="deviance-residuals" class="section level2">
<h2>Deviance residuals</h2>
<p>What does <em>deviance</em> mean? In logistic regression, deviance is a measure of deviation, discrepancy, mismatch between the data and the model. You can think of it as a generalisation of the terms making up the residual sum of squares in simple linear regression. Hence, the deviance measures misfit, <em>badness of fit</em> and so (as it was for the residual sum of squares) the smaller the better!</p>
<p>We get the deviance residuals (always one per case) as follows:</p>
<pre class="r"><code>Dres &lt;- residuals(mdl1, type = &#39;deviance&#39;)</code></pre>
<p>The standardized deviance residuals (zero mean and unit standard deviation) are obtained with</p>
<pre class="r"><code>SDres &lt;- rstandard(mdl1, type = &#39;deviance&#39;)</code></pre>
<p>And the studentized deviance residuals (zero mean and unit standard deviation) are found via:</p>
<pre class="r"><code>StuDres &lt;- rstudent(mdl1, type = &#39;deviance&#39;)</code></pre>
<p>Again, we check whether any residuals are larger than 2 or 3 in absolute value:</p>
<pre class="r"><code>plot(StuDres, ylab = &#39;Studentized Deviance Residuals&#39;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-20-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>All fine!</p>
<p>Again, a plot against the fitted values or the predictors isn’t that useful in logistic regression…</p>
<pre class="r"><code>plot(fitted(mdl1), StuDres, 
     xlab = &#39;Prob. of Success&#39;, ylab = &#39;Studentized Deviance Residuals&#39;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-21-1.png" width="60%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(sen$wais, StuDres, 
     xlab = &#39;Prob. of Success&#39;, ylab = &#39;Studentized Deviance Residuals&#39;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-22-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>So we will check against a binned plot:</p>
<pre class="r"><code>arm::binnedplot(fitted(mdl1), StuDres, 
                xlab = &#39;Prob. of Success&#39;, ylab = &#39;Studentized Deviance Residuals&#39;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-23-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>Again, it seems like everything is fine, no extremely high values.</p>
</div>
<div id="which-residuals-should-i-use" class="section level2">
<h2>Which residuals should I use???</h2>
<p>Both! Compute both, and for visual exploration most of the time it’s fine to use the <strong>deviance</strong> residuals.</p>
<!-- You will get away with it -->
<p>If you use standardized or studentized ones it’s easier to explore extreme values as we expect most residuals to be within -2, 2 or -3, 3.</p>
<p>If you do not provide the <code>type = ...</code> argument to the function <code>residuals()</code>, then it will use the Deviance residuals by default, and so will the functions <code>rstandard()</code> and <code>rstudent()</code>!</p>
</div>
</div>
<div id="influential-values" class="section level1">
<h1>Influential values</h1>
<p>In logistic regression we typically check for influential observations by checking if there are any of the 54 cases in the dataset that have a Cook’s distance greater than 0.5 (moderately influential) or 1 (highly influential):</p>
<pre class="r"><code>plot(cooks.distance(mdl1), ylab = &quot;Cook&#39;s distance&quot;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-24-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>None of the units in the dataset appears to have a Cook’s distance value greater than 0.5, hence there does not seem to be issues with influential points.</p>
</div>
<div id="diagnosing-model-fit" class="section level1">
<h1>Diagnosing model fit</h1>
<div id="drop-in-deviance-test-to-compare-nested-models" class="section level2">
<h2>Drop-in-deviance test to compare nested models</h2>
<p>When moving from linear regression to more advanced and flexible models, testing of goodness of fit is more often done by comparing a model of interest to a simpler one.
The only caveat is that the two models need to be <strong>nested</strong>, i.e. one model needs to be a simplification of the other, and all predictors of one model needs to be within the other.</p>
<p>We want to compare the model we previously fitted against a model where all slopes are 0, i.e. a baseline model:
<span class="math display">\[
\begin{aligned}
M_1 : \qquad\log \left( \frac{p}{1 - p} \right) &amp;= \beta_0 \\
M_2 : \qquad \log \left( \frac{p}{1 - p} \right) &amp;= \beta_0 + \beta_1 x
\end{aligned}
\]</span></p>
<p>The null hypothesis will be that the simpler model is a good fit, while the alternative is that the more complex model is needed.</p>
<p>In R we do the comparison as follows:</p>
<pre class="r"><code>mdl_red &lt;- glm(senility ~ 1, family = binomial, data = sen)

anova(mdl_red, mdl1, test = &#39;Chisq&#39;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: senility ~ 1
## Model 2: senility ~ wais
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)   
## 1        53     61.806                        
## 2        52     51.017  1   10.789 0.001021 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The above code shows the two fitted models</p>
<pre><code>Model 1: senility ~ 1
Model 2: senility ~ wais</code></pre>
<p>And then reports the <em>Residual Deviance</em> of each model, 61.806 and 51.017 respectively. Remember the deviance is the equivalent of residual sum of squares in linear regression.</p>
<p>So, by adding the predictor <span class="math inline">\(x\)</span> = wais to the model, we reduce our deviance from 61.806 to 51.017, i.e. we reduce it by 10.789.
Is this reduction sufficient to be attributed solely to the contribution of the predictor, or could it just be due to random sampling variation? This is what the chi-squared test tells us!</p>
<div class="int">
<p>The reduced model (<span class="math inline">\(M_1\)</span>) has the slope set to zero <span class="math inline">\(\beta_1 = 0\)</span>. Its deviance is obtained by fitting a logistic regression model without any explanatory variables (but including a constant term). This deviance is found to be 61.806, with 53 degrees of freedom.
The deviance statistic from a fit of the model with <span class="math inline">\(x\)</span> = wais included is 51.017, with 52 degrees of freedom. The deviance dropped by 10.789, with a drop of one degree of freedom. The associated p-value from a chi-squared distribution with 1df is <span class="math inline">\(p = .001\)</span>.</p>
<p>There is strong evidence of an association between senility symptoms and scores on the Wechsler Adult Intelligent Scale.</p>
</div>
<!-- Above was Drop-in-deviance GOF test -->
<!-- # Deviance GOF test -->
<!-- mdl_sat = glm(senility ~ factor(1:nrow(sen)), family = binomial, data = sen) -->
<!-- anova(mdl1, mdl_sat, test = 'Chisq') -->
<div class="optional-begin">
Optional More on the deviance<span id="opt-start-210" class="fa fa-plus optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-210&#39;, &#39;opt-start-210&#39;)"></span>
</div>
<div id="opt-body-210" class="optional-body" style="display: none;">
<p>You can get the deviance of a specific model and its corresponding degrees of freedom can be found with the function</p>
<pre class="r"><code>deviance(mdl_red)</code></pre>
<pre><code>## [1] 61.80632</code></pre>
<pre class="r"><code>df.residual(mdl_red)</code></pre>
<pre><code>## [1] 53</code></pre>
<pre class="r"><code>deviance(mdl1)</code></pre>
<pre><code>## [1] 51.01738</code></pre>
<pre class="r"><code>df.residual(mdl1)</code></pre>
<pre><code>## [1] 52</code></pre>
<p>The deviance is computed as -2 * logLik(mdl)</p>
<pre class="r"><code>-2 * logLik(mdl_red)</code></pre>
<pre><code>## &#39;log Lik.&#39; 61.80632 (df=1)</code></pre>
<pre class="r"><code>-2 * logLik(mdl1)</code></pre>
<pre><code>## &#39;log Lik.&#39; 51.01738 (df=2)</code></pre>
</div>
<p class="optional-end">
</p>
</div>
<div id="akaike-and-bayesian-information-criteria" class="section level2">
<h2>Akaike and Bayesian Information Criteria</h2>
<p>Deviance measures lack of fit, and it can be reduced to zero by making the model more and more complex, effectively estimating the value at each single data point.
However, this involves adding more and more predictors, which makes the model more complex (and less interpretable).</p>
<p>Typically, simpler model are preferred when they still explain the data almost as well. This is why information criteria were devised, exactly to account for both the model misfit but also its complexity.</p>
<p><span class="math display">\[
\text{Information Criterion} = \text{Deviance} + \text{Penalty for model complexity}
\]</span></p>
<p>Depending on the chosen penalty, you get different criteria. Two common ones are the Akaike and Bayesian Information Criteria, AIC and BIC respectively:
<span class="math display">\[
\begin{aligned}
\text{AIC} &amp;= \text{Deviance} + 2 p \\
\text{BIC} &amp;= \text{Deviance} + p \log(n)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the sample size and <span class="math inline">\(p\)</span> is the number of regression coefficients in the model. <strong>Models that produce smaller values of these fitting criteria should be preferred.</strong></p>
<p>AIC and BIC differ in their degrees of penalization for number of regression coefficients, with BIC usually favouring models with fewer terms.</p>
<pre class="r"><code>AIC(mdl_red, mdl1)</code></pre>
<pre><code>##         df      AIC
## mdl_red  1 63.80632
## mdl1     2 55.01738</code></pre>
<pre class="r"><code>BIC(mdl_red, mdl1)</code></pre>
<pre><code>##         df      BIC
## mdl_red  1 65.79530
## mdl1     2 58.99535</code></pre>
<p>According to both AIC and BIC the model with wais as a predictor has a lower score, meaning it’s preferable to the other.</p>
<p>In any case, it is important to realise that no criterion can be superior in all situations. Rather, have a look at multiple ones and see if they agree overall.</p>
</div>
</div>
<div id="exercises" class="section level1">
<h1>Exercises</h1>
<p>Kalkhoran et al. (2015)<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> investigated predictors of dual use of cigarettes and smokeless tobacco products. Although their original sample size was large (<span class="math inline">\(n\)</span> = 1324), they were interested in running separate logistic regression analyses within subgroups. Most of the sample used only cigarettes. One of the smaller subgroups contained subjects who used both cigarettes and smokeless tobacco products (<span class="math inline">\(n\)</span>=61). For this problem we focus on this smaller subgroup of dual cigarette and smokeless tobacco users. The dependent variable, Q, is whether the subject made an attempt to quit using tobacco products (0 = no attempt to quit; 1 = attempted to quit). There is one multi-category independent variable, intention to quit, I, with four levels: never intend to quit, may intend to quit but not in the next 6 months, intend to quit in the next 6 months, intend to quit in the next 30 days.</p>
<p>The data can be found at the following link: <a href="https://uoepsy.github.io/data/QuitAttempts.csv" class="uri">https://uoepsy.github.io/data/QuitAttempts.csv</a></p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-29">Table 1: </span>Quit Attempts and Intentions Among Dual Users of Cigarettes and Smokeless Tobacco Products
</caption>
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:left;">
Description
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
S
</td>
<td style="text-align:left;">
Subject
</td>
</tr>
<tr>
<td style="text-align:left;">
Q
</td>
<td style="text-align:left;">
Quitting (0 = No attempt to quit; 1 = Attempted to quit)
</td>
</tr>
<tr>
<td style="text-align:left;">
I
</td>
<td style="text-align:left;">
Intentions to Quit (1 = Never intend to quit; 2 = May intend to quit but not in the next 6 months; 3 = Intend to quit in the next 6 months; 4 = Intend to quit in the next 30 days)
</td>
</tr>
</tbody>
</table>
<div class="question-begin">
Question 1
</div>
<div class="question-body">
<p>Read the data into R.</p>
<p>Perform a preliminary exploratory analysis of the data by computing summary statistics and visualising the distribution of the variables.</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-211" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-211&#39;, &#39;sol-start-211&#39;)"></span>
</div>
<div id="sol-body-211" class="solution-body" style="display: none;">
<pre class="r"><code>qa &lt;- read_csv(&#39;https://uoepsy.github.io/data/QuitAttempts.csv&#39;)
head(qa)</code></pre>
<pre><code>## # A tibble: 6 x 3
##       S     Q     I
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1     0     1
## 2     2     1     3
## 3     3     1     2
## 4     4     0     1
## 5     5     1     2
## 6     6     1     2</code></pre>
<pre class="r"><code>qa %&gt;%
  group_by(I) %&gt;%
  count(Q) %&gt;%
  pivot_wider(names_from = Q, values_from = n) %&gt;%
  mutate(Total = `0` + `1`)</code></pre>
<pre><code>## # A tibble: 4 x 4
## # Groups:   I [4]
##       I   `0`   `1` Total
##   &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1     1     7     1     8
## 2     2     9    17    26
## 3     3     5    18    23
## 4     4     1     4     5</code></pre>
<pre class="r"><code>ggplot(qa, aes(I, fill = factor(Q))) +
  geom_bar() +
  labs(fill = &#39;Attempted quitting?&#39;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-32-1.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question 2
</div>
<div class="question-body">
<p>Fit a logistic regression of Q on I using ordinary logistic regression.</p>
<p>Be sure to treat the independent variable I as categorical rather than as continuous in these analyses (hint: you will need to make sure R will create for you dummy variables for each level of I, rather than treating it as numeric).</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-212" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-212&#39;, &#39;sol-start-212&#39;)"></span>
</div>
<div id="sol-body-212" class="solution-body" style="display: none;">
<p>First, we must make sure <code>I</code> is a factor otherwise R won’t create the dummy variables for us:</p>
<pre class="r"><code>qa$I &lt;- as.factor(qa$I)</code></pre>
<pre class="r"><code>qa_fit1 = glm(Q ~ I, data = qa, family = binomial)
summary(qa_fit1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Q ~ I, family = binomial, data = qa)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7941  -0.5168   0.7002   0.9218   2.0393  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)   -1.946      1.069  -1.820  0.06872 . 
## I2             2.582      1.146   2.253  0.02423 * 
## I3             3.227      1.183   2.729  0.00636 **
## I4             3.332      1.547   2.154  0.03123 * 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 80.648  on 61  degrees of freedom
## Residual deviance: 68.659  on 58  degrees of freedom
## AIC: 76.659
## 
## Number of Fisher Scoring iterations: 4</code></pre>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question 3
</div>
<div class="question-body">
<p>What do you observe from the results?</p>
<p>Interpret the coefficients in the context of the study. You might want to provide confidence intervals for the estimates!</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-213" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-213&#39;, &#39;sol-start-213&#39;)"></span>
</div>
<div id="sol-body-213" class="solution-body" style="display: none;">
<pre class="r"><code>summary(qa_fit1)$coefficients</code></pre>
<pre><code>##              Estimate Std. Error   z value    Pr(&gt;|z|)
## (Intercept) -1.945910   1.069041 -1.820239 0.068722556
## I2           2.581899   1.145767  2.253423 0.024232467
## I3           3.226844   1.182541  2.728737 0.006357734
## I4           3.332205   1.546883  2.154141 0.031229115</code></pre>
<pre class="r"><code>exp(confint(qa_fit1))</code></pre>
<pre><code>##                   2.5 %       97.5 %
## (Intercept) 0.007642342    0.8027604
## I2          1.933949651  268.1718960
## I3          3.416283512  534.1171196
## I4          1.951561286 1181.6932863</code></pre>
<p>First let’s remind ourselves of the levels of our independent variable <code>I</code>.<br />
Intentions to Quit (1 = Never intend to quit; 2 = May intend to quit but not in the next 6 months; 3 = Intend to quit in the next 6 months; 4 = Intend to quit in the next 30 days).</p>
<div class="int">
<p>For a participant who reported that they “never intend to quit,” the odds of attempting to quit was 0.14 (95% CI [0.01–0.8]). That is, for those who reportedly “never intend to quit,” the probability that they attempted quitting was 0.14 times the probability that they didn’t.</p>
<p>Relative to this group, reporting an intention to quit but not in the next 6 months meant that their the odds of quitting increased by a factor of 13.22 (95% CI [1.93–268.17]).</p>
<p>Reporting an intention to quit in the next 6 months increased the odds of quitting by a factor of 25.2 (95% CI [3.42–534.12]).</p>
<p>Finally, intending to quit in the next 30 days was associated with an increase in the odds of attempting to quit by a factor of 28 (95% CI [1.95–1181.69])</p>
</div>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question 4
</div>
<div class="question-body">
<p>Investigate whether or not the studentized deviance residuals raise any concerns about outliers.</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-214" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-214&#39;, &#39;sol-start-214&#39;)"></span>
</div>
<div id="sol-body-214" class="solution-body" style="display: none;">
<pre class="r"><code>plot(residuals(qa_fit1), ylab = &quot;Studentized Deviance Residuals&quot;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-36-1.png" width="60%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(fitted(qa_fit1), residuals(qa_fit1),
     xlab = &quot;Fitted values&quot;, ylab = &quot;Studentized Deviance Residuals&quot;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-36-2.png" width="60%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>arm::binnedplot(fitted(qa_fit1), residuals(qa_fit1),
                ylab = &quot;Average Studentized Deviance Residuals&quot;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-36-3.png" width="60%" style="display: block; margin: auto;" /></p>
<p>The plots do not show residuals substantially larger than 2 in absolute value, hence we do not evidence of outliers.</p>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question 5
</div>
<div class="question-body">
<p>Investigate whether or not there are influential observations.</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-215" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-215&#39;, &#39;sol-start-215&#39;)"></span>
</div>
<div id="sol-body-215" class="solution-body" style="display: none;">
<pre class="r"><code>plot(cooks.distance(qa_fit1), ylab = &quot;Cook&#39;s Distance&quot;)</code></pre>
<p><img src="16_more_about_logistic_files/figure-html/unnamed-chunk-37-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>There are no points with a Cook’s distance larger than 0.5.</p>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question 6
</div>
<div class="question-body">
<p>Perform a Deviance goodness-of-fit test by compare the following nested models:</p>
<p><span class="math display">\[
\begin{aligned}
M_1 &amp;: \qquad \log \left( \frac{p}{1 - p}\right) = \beta_0 \\
M_2 &amp;: \qquad \log \left( \frac{p}{1 - p}\right) = \beta_0 + \beta_1 I2 + \beta_2 I3 + \beta_3 I4
\end{aligned}
\]</span></p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-216" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-216&#39;, &#39;sol-start-216&#39;)"></span>
</div>
<div id="sol-body-216" class="solution-body" style="display: none;">
<pre class="r"><code>qa_null &lt;- glm(Q ~ 1, data = qa, family = binomial)
summary(qa_null)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Q ~ 1, family = binomial, data = qa)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.4395  -1.4395   0.9362   0.9362   0.9362  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   0.5978     0.2654   2.252   0.0243 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 80.648  on 61  degrees of freedom
## Residual deviance: 80.648  on 61  degrees of freedom
## AIC: 82.648
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>anova(qa_null, qa_fit1, test = &#39;Chisq&#39;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Q ~ 1
## Model 2: Q ~ I
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)   
## 1        61     80.648                        
## 2        58     68.659  3   11.989  0.00742 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="int">
<p>At the 5% significance level, the addition of the information about the subjects’ intentions to quit resulted in a significant decrease in model deviance (12). Comparing this with a chi-squared with 3 degrees of freedom results in a <span class="math inline">\(p = .007\)</span>.</p>
<p>Hence, we have strong evidence that the model the subjects’ intention to quit are helpful predictors of whether or not they will attempt quitting in the future.</p>
</div>
</div>
<p class="solution-end">
</p>
<!-- Formatting -->
<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;">

</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This is what you get with <code>plot(model, which = 1)</code><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Gelman, A., &amp; Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (Analytical Methods for Social Research). Cambridge: Cambridge University Press. <a href="doi:10.1017/CBO9780511790942" class="uri">doi:10.1017/CBO9780511790942</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Kalkhoran S, Grana RA, Neilands TB, and Ling PM. Dual use of smokeless tobacco or e-cigarettes with cigarettes and cessation. <em>Am J Health Behav.</em> 2015;39:277–284.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

<link rel="stylesheet" href="https://uoepsy.github.io/assets/css/ccfooter.css" />
<div class="ccfooter"></div>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
