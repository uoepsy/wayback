<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Multiple Comparisons</title>

<script src="site_libs/header-attrs-2.8/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);
e.style.display = ((e.style.display!='none') ? 'none' : 'block');
if(f.classList.contains('fa-plus')) {
    f.classList.add('fa-minus')
    f.classList.remove('fa-plus')
} else {
    f.classList.add('fa-plus')
    f.classList.remove('fa-minus')
}
}
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<link rel="stylesheet" href="assets/style-labs.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"><strong>DAPR2</strong></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fas fa-home"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Simple Linear Regression
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="01_models.html">1/1: Functions and models</a>
    </li>
    <li>
      <a href="02_slr.html">1/2: Simple linear regression</a>
    </li>
    <li>
      <a href="03_slr_model_fit.html">1/3: Model Fit, Standardized Coefficients</a>
    </li>
    <li>
      <a href="04_slr_assumptions.html">1/4: Assumptions &amp; Diagnostics</a>
    </li>
    <li>
      <a href="05_slr_writeup.html">1/5: Writing-up</a>
    </li>
    <li class="dropdown-header">1/6: Break week</li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Multiple Linear Regression
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="06_mlr.html">1/7: Basics</a>
    </li>
    <li>
      <a href="07_mlr_int.html">1/8: Interactions</a>
    </li>
    <li>
      <a href="08_mlr_assumpt.html">1/9: Assumptions &amp; Diagnostics</a>
    </li>
    <li>
      <a href="09_mlr_select.html">1/10: Model Fit, Model Comparison, Model Selection</a>
    </li>
    <li>
      <a href="10_mlr_report.html">1/11: Writing-up</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Experimental Data
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="11_anova.html">2/1: ANOVA</a>
    </li>
    <li>
      <a href="11_anova.html">2/2: ANOVA (again)</a>
    </li>
    <li>
      <a href="12_factorial_anova.html">2/3: 2x2 ANOVA</a>
    </li>
    <li>
      <a href="13_multiplecomp.html">2/4: Multiple Comparisons</a>
    </li>
    <li class="dropdown-header">2/5: No exercises</li>
    <li class="dropdown-header">2/6: Break week</li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Advanced Topics for LM
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="14_bootstrap_reg.html">2/7: Bootstrap</a>
    </li>
    <li>
      <a href="15_binary_logistic.html">2/8: Binary Logistic Regression</a>
    </li>
    <li>
      <a href="16_more_about_logistic.html">2/9: More About Logistic Regression</a>
    </li>
    <li>
      <a href="17_power_regression.html">2/10: Sample Size and Power Analysis</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Help
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="intro_r_rstudio.html">Getting started with R &amp; RStudio</a>
    </li>
  </ul>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Multiple Comparisons</h1>

</div>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This week’s exercises are much lighter than last week, so it might be worth also using it to catch up on previous week’s this term.</p>
<p>Next week :</p>
<p>This week, we’re going to take a brief look at the idea of <strong>multiple comparisons</strong>.</p>
<div id="refreshing-last-week" class="section level2">
<h2>Refreshing last week:</h2>
<p>Last week we learned how to conduct a 2-factor ANOVA. We examined the effects of two variables, Diagnosis (Amnesic, Huntingtons or a Control group) and Task (Grammar, Classification or Recognition).</p>
<p>We incrementally built the 2x2 anova model.
Recall, ANOVA is actually a special case of the linear model in which the predictors are categorical variables, so we can build our model using <code>lm()</code>, and presented slightly differently.<br />
Our full model, including the interaction, looked like this:</p>
<pre class="r"><code>mdl_int &lt;- lm(Score ~ 1 + Diagnosis + Task + Diagnosis:Task, data = cog)
# Alternatively, we could have used the following shorter version:
# mdl_int &lt;- lm(Score ~ 1 + Diagnosis * Task, data = cog)</code></pre>
<p>And to view the typically presented ANOVA table for this model:</p>
<pre class="r"><code>anova(mdl_int)</code></pre>
<table style="width:93%;">
<caption>Analysis of Variance Table</caption>
<colgroup>
<col width="29%" />
<col width="6%" />
<col width="12%" />
<col width="13%" />
<col width="13%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Df</th>
<th align="center">Sum Sq</th>
<th align="center">Mean Sq</th>
<th align="center">F value</th>
<th align="center">Pr(&gt;F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Diagnosis</strong></td>
<td align="center">2</td>
<td align="center">5250</td>
<td align="center">2625</td>
<td align="center">16.64</td>
<td align="center">7.64e-06</td>
</tr>
<tr class="even">
<td align="center"><strong>Task</strong></td>
<td align="center">2</td>
<td align="center">5250</td>
<td align="center">2625</td>
<td align="center">16.64</td>
<td align="center">7.64e-06</td>
</tr>
<tr class="odd">
<td align="center"><strong>Diagnosis:Task</strong></td>
<td align="center">4</td>
<td align="center">5000</td>
<td align="center">1250</td>
<td align="center">7.923</td>
<td align="center">0.0001092</td>
</tr>
<tr class="even">
<td align="center"><strong>Residuals</strong></td>
<td align="center">36</td>
<td align="center">5680</td>
<td align="center">157.8</td>
<td align="center">NA</td>
<td align="center">NA</td>
</tr>
</tbody>
</table>
<div class="int">
<p>The interaction between diagnosis and task is significant. At the 5% level, the probability of obtaining an F-statistic as large as 7.92 or larger, if there was no interaction effect, is &lt;.001. This provides very strong evidence against the null hypothesis that effect of task is constant across the different diagnoses.</p>
</div>
<p>(remember: in the presence of a significant interaction it <strong>does not make sense</strong> to interpret the main effects as their interpretation changes with the level of the other factor)</p>
</div>
<div id="anova-is-an-omnibus-test" class="section level2">
<h2>ANOVA is an “omnibus” test</h2>
<p>The results from an ANOVA are often called an ‘omnibus’ test, because we are testing the null hypothesis that a set of group means are equal (or in the interaction case, that the differences between group means are equal across some other factor).</p>
<p>If you have found Semester 1 materials on linear models a bit more intuitive than ANOVA, another way to think of it is that we are testing the improvement of model fit between a full and a reduced model.<br />
For instance, our significant interaction <span class="math inline">\(F(4, 36) = 7.9225, p &lt; .001\)</span> can also be obtained by comparing the nested models (one with the interaction vs one without):</p>
<pre class="r"><code>mdl_add &lt;- lm(Score ~ 1 + Diagnosis + Task, data = cog)
mdl_int &lt;- lm(Score ~ 1 + Diagnosis + Task + Diagnosis:Task, data = cog)
anova(mdl_add, mdl_int)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Score ~ 1 + Diagnosis + Task
## Model 2: Score ~ 1 + Diagnosis + Task + Diagnosis:Task
##   Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1     40 10680                                  
## 2     36  5680  4      5000 7.9225 0.0001092 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>But it is common to want to know more about the details of such an effect. What groups differ, and by how much?</p>
<p>The traditional approach is to conduct an ANOVA, and then <em>only</em> ask this sort of follow-up question <em>if</em> obtaining a significant omnibus test.<br />
You might think of it as:</p>
<p>Question 1: Omnibus: “Are there any differences in group means?”<br />
Question 2: Comparisons: “<em>What</em> are the differences and between which groups?”</p>
<p>If your answer to 1 is “No,” then it doesn’t make much sense to ask question 2.</p>
</div>
<div id="multiple-comparisons" class="section level2">
<h2>Multiple Comparisons</h2>
<p>In last week’s exercises we began to look at how we compare different groups, by using contrast analysis to conduct tests of specific comparisons between groups. We also saw how we might conduct “pairwise comparisons,” where we test all possible pairs of group means within a given set.</p>
<p>For instance, we compares the means of the different diagnosis groups for each task:</p>
<pre class="r"><code>emm_task &lt;- emmeans(mdl_int, ~ Diagnosis | Task)
contr_task &lt;- contrast(emm_task, method = &#39;pairwise&#39;, 
                       adjust = &quot;bonferroni&quot;)
contr_task</code></pre>
<p>or we might test all different combinations of task and diagnosis group (if that was something we were theoretically interested in, which is unlikely!) which would equate to conducting 36 comparisons!</p>
<pre class="r"><code>emm_task &lt;- emmeans(mdl_int, ~ Diagnosis * Task)
contr_task &lt;- contrast(emm_task, method = &#39;pairwise&#39;, 
                       adjust = &quot;bonferroni&quot;)
contr_task</code></pre>
<pre><code>##  contrast                                             estimate   SE df t.ratio
##  control grammar - amnesic grammar                          20 7.94 36  2.518 
##  control grammar - huntingtons grammar                      40 7.94 36  5.035 
##  control grammar - control classification                    0 7.94 36  0.000 
##  control grammar - amnesic classification                   10 7.94 36  1.259 
##  control grammar - huntingtons classification               35 7.94 36  4.406 
##  control grammar - control recognition                     -15 7.94 36 -1.888 
##  control grammar - amnesic recognition                      15 7.94 36  1.888 
##  control grammar - huntingtons recognition                 -15 7.94 36 -1.888 
##  amnesic grammar - huntingtons grammar                      20 7.94 36  2.518 
##  amnesic grammar - control classification                  -20 7.94 36 -2.518 
##  amnesic grammar - amnesic classification                  -10 7.94 36 -1.259 
##  amnesic grammar - huntingtons classification               15 7.94 36  1.888 
##  amnesic grammar - control recognition                     -35 7.94 36 -4.406 
##  amnesic grammar - amnesic recognition                      -5 7.94 36 -0.629 
##  amnesic grammar - huntingtons recognition                 -35 7.94 36 -4.406 
##  huntingtons grammar - control classification              -40 7.94 36 -5.035 
##  huntingtons grammar - amnesic classification              -30 7.94 36 -3.776 
##  huntingtons grammar - huntingtons classification           -5 7.94 36 -0.629 
##  huntingtons grammar - control recognition                 -55 7.94 36 -6.923 
##  huntingtons grammar - amnesic recognition                 -25 7.94 36 -3.147 
##  huntingtons grammar - huntingtons recognition             -55 7.94 36 -6.923 
##  control classification - amnesic classification            10 7.94 36  1.259 
##  control classification - huntingtons classification        35 7.94 36  4.406 
##  control classification - control recognition              -15 7.94 36 -1.888 
##  control classification - amnesic recognition               15 7.94 36  1.888 
##  control classification - huntingtons recognition          -15 7.94 36 -1.888 
##  amnesic classification - huntingtons classification        25 7.94 36  3.147 
##  amnesic classification - control recognition              -25 7.94 36 -3.147 
##  amnesic classification - amnesic recognition                5 7.94 36  0.629 
##  amnesic classification - huntingtons recognition          -25 7.94 36 -3.147 
##  huntingtons classification - control recognition          -50 7.94 36 -6.294 
##  huntingtons classification - amnesic recognition          -20 7.94 36 -2.518 
##  huntingtons classification - huntingtons recognition      -50 7.94 36 -6.294 
##  control recognition - amnesic recognition                  30 7.94 36  3.776 
##  control recognition - huntingtons recognition               0 7.94 36  0.000 
##  amnesic recognition - huntingtons recognition             -30 7.94 36 -3.776 
##  p.value
##  0.5907 
##  0.0005 
##  1.0000 
##  1.0000 
##  0.0033 
##  1.0000 
##  1.0000 
##  1.0000 
##  0.5907 
##  0.5907 
##  1.0000 
##  1.0000 
##  0.0033 
##  1.0000 
##  0.0033 
##  0.0005 
##  0.0207 
##  1.0000 
##  &lt;.0001 
##  0.1190 
##  &lt;.0001 
##  1.0000 
##  0.0033 
##  1.0000 
##  1.0000 
##  1.0000 
##  0.1190 
##  0.1190 
##  1.0000 
##  0.1190 
##  &lt;.0001 
##  0.5907 
##  &lt;.0001 
##  0.0207 
##  1.0000 
##  0.0207 
## 
## P value adjustment: bonferroni method for 36 tests</code></pre>
<div class="optional-begin">
36? how do we know there are 36?<span id="opt-start-163" class="fa fa-plus optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-163&#39;, &#39;opt-start-163&#39;)"></span>
</div>
<div id="opt-body-163" class="optional-body" style="display: none;">
<p>There are 3 diagnosis groups, and 3 tasks, meaning there are 9 different group means.<br />
All possible pairwise comparisons would is all different possible combinations of 2 from a set of 9.<br />
We can work this out using the rule:</p>
<p>$$
_nC_r =  \
<span class="math display">\[\begin{align} \\
&amp; \text{Where:} \\
&amp; n = \text{total number in the set} \\
&amp; r = \text{number chosen} \\
&amp; _nC_r = \text{number of combinations of r from n} \\
\end{align}\]</span></p>
<p>$$
In R:</p>
<pre class="r"><code>factorial(9)/(factorial(2)*(factorial(9-2)))</code></pre>
<pre><code>## [1] 36</code></pre>
<p>Or, easier still:</p>
<pre class="r"><code>dim(combn(9, 2))</code></pre>
<pre><code>## [1]  2 36</code></pre>
</div>
<p class="optional-end">
</p>
</div>
</div>
<div id="why-does-the-number-of-tests-matter" class="section level1">
<h1>Why does the number of tests matter?</h1>
<p>As discussed briefly in last week’s exercises, we will ideally ensure that our error rate is 0.05 (i.e., the chance that we reject a null hypothesis when it actually true, is 5%).</p>
<div class="optional-begin">
refresher on making errors in hypothesis tests<span id="opt-start-164" class="fa fa-plus optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-164&#39;, &#39;opt-start-164&#39;)"></span>
</div>
<div id="opt-body-164" class="optional-body" style="display: none;">
<p>Think back to <a href="https://uoe-psychology.github.io/uoe_psystats/dapr1/book/chap-typeerror.html">“Type 1 errors” from DAPR1</a> - when we conduct an hypothesis test, and we set <span class="math inline">\(\alpha=0.05\)</span>, we will reject the null hypothesis <span class="math inline">\(H_0\)</span> when we find a <span class="math inline">\(p &lt; .05\)</span>. Now remember what a <span class="math inline">\(p\)</span>-value represents - it is the chance of observing a statistic at least as extreme as the one we do have, assuming the null hypothesis to be true. This means that <em>if</em> <span class="math inline">\(H_0\)</span> <strong>is</strong> true, then we will still observe a <span class="math inline">\(p &lt; .05\)</span> 5% of the time. So our chance of making this error = the threshold (<span class="math inline">\(\alpha\)</span>) at which below a p-value results in us rejecting <span class="math inline">\(H_0\)</span>.</p>
</div>
<p class="optional-end">
</p>
<p>But this error-rate applies to each statistical hypothesis we test. So if we conduct an experiment in which we plan on conducting lots of tests of different comparisons, the chance of an error being made increases substantially. Across the family of tests performed that chance will be much higher than 5%.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>Each test conducted at <span class="math inline">\(\alpha = 0.05\)</span> has a 0.05 (or 5%) probability of Type I error (wrongly rejecting the null hypothesis). If we do 9 tests, that experimentwise error rate is <span class="math inline">\(\alpha_{ew} \leq 9 \times 0.05\)</span>, where 9 is the number of comparisons made as part of the experiment.
Thus, if nine <strong>independent</strong> comparisons were made at the <span class="math inline">\(\alpha = 0.05\)</span> level, the experimentwise Type I error rate <span class="math inline">\(\alpha_{ew}\)</span> would be at most <span class="math inline">\(9 \times 0.05 = 0.45\)</span>. That is, we could wrongly reject the null hypothesis on average 45 times out of 100.
To make this more confusing, many of the tests in a family are not <strong>independent</strong> (see the lecture slides for the calculation of error rate for dependent tests).</p>
<p>Here, we go through some of the different options available to us to control, or ‘correct’ for this problem. The first we look at was used in the solutions to last week’s exercises, and is perhaps the most well-known.</p>
</div>
<div id="bonferroni" class="section level1">
<h1>Bonferroni</h1>
<div class="yellow">
<p><strong>Bonferroni</strong></p>
<ul>
<li>Use Bonferroni’s method when you are interested in a small number of planned contrasts (or pairwise comparisons).</li>
<li>Bonferroni’s method is to divide alpha by the number of tests/confidence intervals.</li>
<li>Assumes that all comparisons are independent of one another.<br />
</li>
<li>It sacrifices slightly more power than Tukey’s method (discussed below), but it can be applied to any set of contrasts or linear combinations (i.e., it is useful in more situations than Tukey).</li>
<li>It is usually better than Tukey if we want to do a small number of planned comparisons.</li>
</ul>
</div>
<div class="question-begin">
Question 1
</div>
<div class="question-body">
<p>Load the data from last week, and re-acquaint yourself with it.
Provide a plot of the Diagnosis*Task group mean scores.<br />
The data is at <a href="https://uoepsy.github.io/data/cognitive_experiment.csv" class="uri">https://uoepsy.github.io/data/cognitive_experiment.csv</a>.</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-165" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-165&#39;, &#39;sol-start-165&#39;)"></span>
</div>
<div id="sol-body-165" class="solution-body" style="display: none;">
<pre class="r"><code>cog &lt;- read_csv(&#39;https://uoepsy.github.io/data/cognitive_experiment.csv&#39;)
# head(df)

cog$Diagnosis &lt;- factor(cog$Diagnosis, 
                       labels = c(&quot;amnesic&quot;, &quot;huntingtons&quot;, &quot;control&quot;),
                       ordered = FALSE)

cog$Task &lt;- factor(cog$Task, 
                  labels = c(&quot;grammar&quot;, &quot;classification&quot;, &quot;recognition&quot;), 
                  ordered = FALSE)

cog$Diagnosis &lt;- fct_relevel(cog$Diagnosis, &quot;control&quot;)

cog &lt;- cog %&gt;%
    rename(Score = Y)</code></pre>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question 2
</div>
<div class="question-body">
<p>Fit the interaction model, using <code>lm()</code>.
Pass your model to the <code>anova()</code> function, to remind yourself that there is a significant interaction present.</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-166" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-166&#39;, &#39;sol-start-166&#39;)"></span>
</div>
<div id="sol-body-166" class="solution-body" style="display: none;">
<pre class="r"><code>mdl_int &lt;- lm(Score ~ Task*Diagnosis, data = cog)
anova(mdl_int)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: Score
##                Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## Task            2   5250 2625.00 16.6373  7.64e-06 ***
## Diagnosis       2   5250 2625.00 16.6373  7.64e-06 ***
## Task:Diagnosis  4   5000 1250.00  7.9225 0.0001092 ***
## Residuals      36   5680  157.78                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question 3
</div>
<div class="question-body">
<p>There are various ways to make nice tables in RMarkdown.<br />
Some of the most well known are:</p>
<ul>
<li>The <strong>knitr</strong> package has <code>kable()</code><br />
</li>
<li>The <strong>pander</strong> package has <code>pander()</code></li>
</ul>
<p>Pick one (or find go googling and find a package you like the look of), install the package (if you don’t already have it), then try to create a nice pretty ANOVA table rather than the one given by <code>anova(model)</code>.</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-167" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-167&#39;, &#39;sol-start-167&#39;)"></span>
</div>
<div id="sol-body-167" class="solution-body" style="display: none;">
<pre class="r"><code>library(knitr)
kable(anova(mdl_int))</code></pre>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Df
</th>
<th style="text-align:right;">
Sum Sq
</th>
<th style="text-align:right;">
Mean Sq
</th>
<th style="text-align:right;">
F value
</th>
<th style="text-align:right;">
Pr(&gt;F)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Task
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
5250
</td>
<td style="text-align:right;">
2625.0000
</td>
<td style="text-align:right;">
16.637324
</td>
<td style="text-align:right;">
0.0000076
</td>
</tr>
<tr>
<td style="text-align:left;">
Diagnosis
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
5250
</td>
<td style="text-align:right;">
2625.0000
</td>
<td style="text-align:right;">
16.637324
</td>
<td style="text-align:right;">
0.0000076
</td>
</tr>
<tr>
<td style="text-align:left;">
Task:Diagnosis
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
5000
</td>
<td style="text-align:right;">
1250.0000
</td>
<td style="text-align:right;">
7.922535
</td>
<td style="text-align:right;">
0.0001092
</td>
</tr>
<tr>
<td style="text-align:left;">
Residuals
</td>
<td style="text-align:right;">
36
</td>
<td style="text-align:right;">
5680
</td>
<td style="text-align:right;">
157.7778
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>library(pander)
pander(anova(mdl_int))</code></pre>
<table style="width:93%;">
<caption>Analysis of Variance Table</caption>
<colgroup>
<col width="29%" />
<col width="6%" />
<col width="12%" />
<col width="13%" />
<col width="13%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Df</th>
<th align="center">Sum Sq</th>
<th align="center">Mean Sq</th>
<th align="center">F value</th>
<th align="center">Pr(&gt;F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Task</strong></td>
<td align="center">2</td>
<td align="center">5250</td>
<td align="center">2625</td>
<td align="center">16.64</td>
<td align="center">7.64e-06</td>
</tr>
<tr class="even">
<td align="center"><strong>Diagnosis</strong></td>
<td align="center">2</td>
<td align="center">5250</td>
<td align="center">2625</td>
<td align="center">16.64</td>
<td align="center">7.64e-06</td>
</tr>
<tr class="odd">
<td align="center"><strong>Task:Diagnosis</strong></td>
<td align="center">4</td>
<td align="center">5000</td>
<td align="center">1250</td>
<td align="center">7.923</td>
<td align="center">0.0001092</td>
</tr>
<tr class="even">
<td align="center"><strong>Residuals</strong></td>
<td align="center">36</td>
<td align="center">5680</td>
<td align="center">157.8</td>
<td align="center">NA</td>
<td align="center">NA</td>
</tr>
</tbody>
</table>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question 4
</div>
<div class="question-body">
<p>As in the previous week’s exercises, let us suppose that we are specifically interested in comparisons of the mean score <em>across the different diagnosis groups for a given task</em>.</p>
<p>Edit the code below to obtain the pairwise comparisons of diagnosis groups for each task. Use the Bonferroni method to adjust for multiple comparisons, and then obtain confidence intervals.</p>
<pre class="r"><code>library(emmeans)
emm_task &lt;- emmeans(mdl_int, ? )
contr_task &lt;- contrast(emm_task, method = ?, adjust = ? )</code></pre>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-168" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-168&#39;, &#39;sol-start-168&#39;)"></span>
</div>
<div id="sol-body-168" class="solution-body" style="display: none;">
<pre class="r"><code>emm_task &lt;- emmeans(mdl_int, ~ Diagnosis | Task)
contr_task &lt;- contrast(emm_task, method = &quot;pairwise&quot;, adjust=&quot;bonferroni&quot;)
contr_task</code></pre>
<pre><code>## Task = grammar:
##  contrast              estimate   SE df t.ratio p.value
##  control - amnesic           20 7.94 36  2.518  0.0492 
##  control - huntingtons       40 7.94 36  5.035  &lt;.0001 
##  amnesic - huntingtons       20 7.94 36  2.518  0.0492 
## 
## Task = classification:
##  contrast              estimate   SE df t.ratio p.value
##  control - amnesic           10 7.94 36  1.259  0.6486 
##  control - huntingtons       35 7.94 36  4.406  0.0003 
##  amnesic - huntingtons       25 7.94 36  3.147  0.0099 
## 
## Task = recognition:
##  contrast              estimate   SE df t.ratio p.value
##  control - amnesic           30 7.94 36  3.776  0.0017 
##  control - huntingtons        0 7.94 36  0.000  1.0000 
##  amnesic - huntingtons      -30 7.94 36 -3.776  0.0017 
## 
## P value adjustment: bonferroni method for 3 tests</code></pre>
<pre class="r"><code>confint(contr_task)</code></pre>
<pre><code>## Task = grammar:
##  contrast              estimate   SE df lower.CL upper.CL
##  control - amnesic           20 7.94 36   0.0517     39.9
##  control - huntingtons       40 7.94 36  20.0517     59.9
##  amnesic - huntingtons       20 7.94 36   0.0517     39.9
## 
## Task = classification:
##  contrast              estimate   SE df lower.CL upper.CL
##  control - amnesic           10 7.94 36  -9.9483     29.9
##  control - huntingtons       35 7.94 36  15.0517     54.9
##  amnesic - huntingtons       25 7.94 36   5.0517     44.9
## 
## Task = recognition:
##  contrast              estimate   SE df lower.CL upper.CL
##  control - amnesic           30 7.94 36  10.0517     49.9
##  control - huntingtons        0 7.94 36 -19.9483     19.9
##  amnesic - huntingtons      -30 7.94 36 -49.9483    -10.1
## 
## Confidence level used: 0.95 
## Conf-level adjustment: bonferroni method for 3 estimates</code></pre>
</div>
<p class="solution-end">
</p>
<div class="frame">
<p><strong>adjusting <span class="math inline">\(\alpha\)</span>, adjusting p</strong></p>
<p>In the lecture we talked about adjusting the <span class="math inline">\(\alpha\)</span> level (i.e., instead of determining significance at <span class="math inline">\(p &lt; .05\)</span>, we might adjust and determine a result to be statistically significant if <span class="math inline">\(p &lt; .005\)</span>, depending on how many tests are in our family of tests).</p>
<p>Note what the functions in R do is adjust the <span class="math inline">\(p\)</span>-value, rather than the <span class="math inline">\(\alpha\)</span>.
The Bonferroni method simply multiplies the ‘raw’ p-value by the number of the tests.</p>
</div>
<div class="question-begin">
Question 5
</div>
<div class="question-body">
<p>In question 4, above, there are 9 tests being performed, but there are 3 in each ‘family’ (each <code>Task</code>).</p>
<p>Try changing your answer to question 4 to use <code>adjust = "none"</code>, rather than <code>"bonferroni"</code>, and confirm that the p-values are 1/3 of the size.</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-169" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-169&#39;, &#39;sol-start-169&#39;)"></span>
</div>
<div id="sol-body-169" class="solution-body" style="display: none;">
<p>The first Bonferroni adjusted p-value is 0.0492.</p>
<pre class="r"><code>0.0492/3</code></pre>
<pre><code>## [1] 0.0164</code></pre>
<p>Let’s check that this is the raw p-value:</p>
<pre class="r"><code>contrast(emm_task, method = &quot;pairwise&quot;, adjust=&quot;none&quot;)</code></pre>
<pre><code>## Task = grammar:
##  contrast              estimate   SE df t.ratio p.value
##  control - amnesic           20 7.94 36  2.518  0.0164 
##  control - huntingtons       40 7.94 36  5.035  &lt;.0001 
##  amnesic - huntingtons       20 7.94 36  2.518  0.0164 
## 
## Task = classification:
##  contrast              estimate   SE df t.ratio p.value
##  control - amnesic           10 7.94 36  1.259  0.2162 
##  control - huntingtons       35 7.94 36  4.406  0.0001 
##  amnesic - huntingtons       25 7.94 36  3.147  0.0033 
## 
## Task = recognition:
##  contrast              estimate   SE df t.ratio p.value
##  control - amnesic           30 7.94 36  3.776  0.0006 
##  control - huntingtons        0 7.94 36  0.000  1.0000 
##  amnesic - huntingtons      -30 7.94 36 -3.776  0.0006</code></pre>
</div>
<p class="solution-end">
</p>
</div>
<div id="šídák" class="section level1">
<h1>Šídák</h1>
<div class="yellow">
<p><strong>Šídák</strong></p>
<ul>
<li>(A bit) more powerful than the Bonferroni method.</li>
<li>Assumes that all comparisons are independent of one another.</li>
<li>Less common than Bonferroni method, largely because it is more difficult to calculate (not a problem now we have computers).</li>
</ul>
</div>
<div class="question-begin">
Question 6
</div>
<div class="question-body">
<p>The Sidak approach is slightly less conservative than the Bonferroni adjustment.<br />
Doing this with the <strong>emmeans</strong> package is easy, can you guess how?</p>
<p><strong>Hint:</strong> you just have to change the <code>adjust</code> argument in <code>contrast()</code> function.</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-170" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-170&#39;, &#39;sol-start-170&#39;)"></span>
</div>
<div id="sol-body-170" class="solution-body" style="display: none;">
<pre class="r"><code>contrast(emm_task, method = &quot;pairwise&quot;, adjust = &quot;sidak&quot;)</code></pre>
<pre><code>## Task = grammar:
##  contrast              estimate   SE df t.ratio p.value
##  control - amnesic           20 7.94 36  2.518  0.0484 
##  control - huntingtons       40 7.94 36  5.035  &lt;.0001 
##  amnesic - huntingtons       20 7.94 36  2.518  0.0484 
## 
## Task = classification:
##  contrast              estimate   SE df t.ratio p.value
##  control - amnesic           10 7.94 36  1.259  0.5185 
##  control - huntingtons       35 7.94 36  4.406  0.0003 
##  amnesic - huntingtons       25 7.94 36  3.147  0.0099 
## 
## Task = recognition:
##  contrast              estimate   SE df t.ratio p.value
##  control - amnesic           30 7.94 36  3.776  0.0017 
##  control - huntingtons        0 7.94 36  0.000  1.0000 
##  amnesic - huntingtons      -30 7.94 36 -3.776  0.0017 
## 
## P value adjustment: sidak method for 3 tests</code></pre>
</div>
<p class="solution-end">
</p>
</div>
<div id="tukey" class="section level1">
<h1>Tukey</h1>
<div class="yellow">
<p><strong>Tukey</strong></p>
<ul>
<li>It specifies an exact family significance level for comparing all pairs of treatment means.<br />
</li>
<li>Use Tukey’s method when you are interested in all (or most) pairwise comparisons of means.</li>
</ul>
</div>
<p>As for Šídák, In R we can easily change to Tukey. For instance, if we wanted to conduct pairwise comparisons of the scores of different Diagnosis groups on different Task types (i.e., the interaction):</p>
<pre class="r"><code>emm_task &lt;- emmeans(mdl_int, ~ Diagnosis*Task)
contr_task &lt;- contrast(emm_task, method = &quot;pairwise&quot;, adjust=&quot;tukey&quot;)
contr_task</code></pre>
<pre><code>##  contrast                                             estimate   SE df t.ratio
##  control grammar - amnesic grammar                          20 7.94 36  2.518 
##  control grammar - huntingtons grammar                      40 7.94 36  5.035 
##  control grammar - control classification                    0 7.94 36  0.000 
##  control grammar - amnesic classification                   10 7.94 36  1.259 
##  control grammar - huntingtons classification               35 7.94 36  4.406 
##  control grammar - control recognition                     -15 7.94 36 -1.888 
##  control grammar - amnesic recognition                      15 7.94 36  1.888 
##  control grammar - huntingtons recognition                 -15 7.94 36 -1.888 
##  amnesic grammar - huntingtons grammar                      20 7.94 36  2.518 
##  amnesic grammar - control classification                  -20 7.94 36 -2.518 
##  amnesic grammar - amnesic classification                  -10 7.94 36 -1.259 
##  amnesic grammar - huntingtons classification               15 7.94 36  1.888 
##  amnesic grammar - control recognition                     -35 7.94 36 -4.406 
##  amnesic grammar - amnesic recognition                      -5 7.94 36 -0.629 
##  amnesic grammar - huntingtons recognition                 -35 7.94 36 -4.406 
##  huntingtons grammar - control classification              -40 7.94 36 -5.035 
##  huntingtons grammar - amnesic classification              -30 7.94 36 -3.776 
##  huntingtons grammar - huntingtons classification           -5 7.94 36 -0.629 
##  huntingtons grammar - control recognition                 -55 7.94 36 -6.923 
##  huntingtons grammar - amnesic recognition                 -25 7.94 36 -3.147 
##  huntingtons grammar - huntingtons recognition             -55 7.94 36 -6.923 
##  control classification - amnesic classification            10 7.94 36  1.259 
##  control classification - huntingtons classification        35 7.94 36  4.406 
##  control classification - control recognition              -15 7.94 36 -1.888 
##  control classification - amnesic recognition               15 7.94 36  1.888 
##  control classification - huntingtons recognition          -15 7.94 36 -1.888 
##  amnesic classification - huntingtons classification        25 7.94 36  3.147 
##  amnesic classification - control recognition              -25 7.94 36 -3.147 
##  amnesic classification - amnesic recognition                5 7.94 36  0.629 
##  amnesic classification - huntingtons recognition          -25 7.94 36 -3.147 
##  huntingtons classification - control recognition          -50 7.94 36 -6.294 
##  huntingtons classification - amnesic recognition          -20 7.94 36 -2.518 
##  huntingtons classification - huntingtons recognition      -50 7.94 36 -6.294 
##  control recognition - amnesic recognition                  30 7.94 36  3.776 
##  control recognition - huntingtons recognition               0 7.94 36  0.000 
##  amnesic recognition - huntingtons recognition             -30 7.94 36 -3.776 
##  p.value
##  0.2575 
##  0.0004 
##  1.0000 
##  0.9367 
##  0.0026 
##  0.6257 
##  0.6257 
##  0.6257 
##  0.2575 
##  0.2575 
##  0.9367 
##  0.6257 
##  0.0026 
##  0.9993 
##  0.0026 
##  0.0004 
##  0.0149 
##  0.9993 
##  &lt;.0001 
##  0.0711 
##  &lt;.0001 
##  0.9367 
##  0.0026 
##  0.6257 
##  0.6257 
##  0.6257 
##  0.0711 
##  0.0711 
##  0.9993 
##  0.0711 
##  &lt;.0001 
##  0.2575 
##  &lt;.0001 
##  0.0149 
##  1.0000 
##  0.0149 
## 
## P value adjustment: tukey method for comparing a family of 9 estimates</code></pre>
<p>We can also use the following, which doesn’t require the <strong>emmeans</strong> package. You might see this when you look online for resources. The <code>aov()</code> function is fitting an ANOVA model, and then <code>TukeyHSD()</code> compares between Diagnosis group; between Task type; and between Diagnosis*Task.<br />
Run the code below yourself to see the ouput.</p>
<pre class="r"><code>TukeyHSD(aov(Score ~ Diagnosis * Task, data = cog))</code></pre>
</div>
<div id="scheffe" class="section level1">
<h1>Scheffe</h1>
<div class="yellow">
<p><strong>Scheffe</strong></p>
<ul>
<li>It is the most conservative (least powerful) of all tests.</li>
<li>It controls the family alpha level for testing all possible contrasts.</li>
<li>It should be used if you have not planned contrasts in advance.</li>
<li>For testing pairs of treatment means it is too conservative (you should use Bonferroni or Šídák).</li>
</ul>
</div>
<pre class="r"><code>emm_task &lt;- emmeans(mdl_int, ~ Diagnosis * Task)
contr_task &lt;- contrast(emm_task, method = &quot;pairwise&quot;, adjust=&quot;scheffe&quot;)
contr_task</code></pre>
<pre><code>##  contrast                                             estimate   SE df t.ratio
##  control grammar - amnesic grammar                          20 7.94 36  2.518 
##  control grammar - huntingtons grammar                      40 7.94 36  5.035 
##  control grammar - control classification                    0 7.94 36  0.000 
##  control grammar - amnesic classification                   10 7.94 36  1.259 
##  control grammar - huntingtons classification               35 7.94 36  4.406 
##  control grammar - control recognition                     -15 7.94 36 -1.888 
##  control grammar - amnesic recognition                      15 7.94 36  1.888 
##  control grammar - huntingtons recognition                 -15 7.94 36 -1.888 
##  amnesic grammar - huntingtons grammar                      20 7.94 36  2.518 
##  amnesic grammar - control classification                  -20 7.94 36 -2.518 
##  amnesic grammar - amnesic classification                  -10 7.94 36 -1.259 
##  amnesic grammar - huntingtons classification               15 7.94 36  1.888 
##  amnesic grammar - control recognition                     -35 7.94 36 -4.406 
##  amnesic grammar - amnesic recognition                      -5 7.94 36 -0.629 
##  amnesic grammar - huntingtons recognition                 -35 7.94 36 -4.406 
##  huntingtons grammar - control classification              -40 7.94 36 -5.035 
##  huntingtons grammar - amnesic classification              -30 7.94 36 -3.776 
##  huntingtons grammar - huntingtons classification           -5 7.94 36 -0.629 
##  huntingtons grammar - control recognition                 -55 7.94 36 -6.923 
##  huntingtons grammar - amnesic recognition                 -25 7.94 36 -3.147 
##  huntingtons grammar - huntingtons recognition             -55 7.94 36 -6.923 
##  control classification - amnesic classification            10 7.94 36  1.259 
##  control classification - huntingtons classification        35 7.94 36  4.406 
##  control classification - control recognition              -15 7.94 36 -1.888 
##  control classification - amnesic recognition               15 7.94 36  1.888 
##  control classification - huntingtons recognition          -15 7.94 36 -1.888 
##  amnesic classification - huntingtons classification        25 7.94 36  3.147 
##  amnesic classification - control recognition              -25 7.94 36 -3.147 
##  amnesic classification - amnesic recognition                5 7.94 36  0.629 
##  amnesic classification - huntingtons recognition          -25 7.94 36 -3.147 
##  huntingtons classification - control recognition          -50 7.94 36 -6.294 
##  huntingtons classification - amnesic recognition          -20 7.94 36 -2.518 
##  huntingtons classification - huntingtons recognition      -50 7.94 36 -6.294 
##  control recognition - amnesic recognition                  30 7.94 36  3.776 
##  control recognition - huntingtons recognition               0 7.94 36  0.000 
##  amnesic recognition - huntingtons recognition             -30 7.94 36 -3.776 
##  p.value
##  0.6128 
##  0.0080 
##  1.0000 
##  0.9894 
##  0.0329 
##  0.8852 
##  0.8852 
##  0.8852 
##  0.6128 
##  0.6128 
##  0.9894 
##  0.8852 
##  0.0329 
##  0.9999 
##  0.0329 
##  0.0080 
##  0.1131 
##  0.9999 
##  0.0001 
##  0.3060 
##  0.0001 
##  0.9894 
##  0.0329 
##  0.8852 
##  0.8852 
##  0.8852 
##  0.3060 
##  0.3060 
##  0.9999 
##  0.3060 
##  0.0003 
##  0.6128 
##  0.0003 
##  0.1131 
##  1.0000 
##  0.1131 
## 
## P value adjustment: scheffe method with rank 8</code></pre>
</div>
<div id="when-to-use-which" class="section level1">
<h1>When to use which</h1>
<p>For ease of scrolling, we have provided the bulletpoints for each correction below in one place:</p>
<div class="yellow">
<p><strong>Bonferroni</strong></p>
<ul>
<li>Use Bonferroni’s method when you are interested in a small number of planned contrasts (or pairwise comparisons).</li>
<li>Bonferroni’s method is to divide alpha by the number of tests/confidence intervals.</li>
<li>Assumes that all comparisons are independent of one another.<br />
</li>
<li>It sacrifices slightly more power than Tukey’s method (discussed below), but it can be applied to any set of contrasts or linear combinations (i.e., it is useful in more situations than Tukey).</li>
<li>It is usually better than Tukey if we want to do a small number of planned comparisons.</li>
</ul>
<p><strong>Šídák</strong></p>
<ul>
<li>(A bit) more powerful than the Bonferroni method.</li>
<li>Assumes that all comparisons are independent of one another.</li>
<li>Less common than Bonferroni method, largely because it is more difficult to calculate (not a problem now we have computers).</li>
</ul>
<p><strong>Tukey</strong></p>
<ul>
<li>It specifies an exact family significance level for comparing all pairs of treatment means.<br />
</li>
<li>Use Tukey’s method when you are interested in all (or most) pairwise comparisons of means.</li>
</ul>
<p><strong>Scheffe</strong></p>
<ul>
<li>It is the most conservative (least powerful) of all tests.</li>
<li>It controls the family alpha level for testing all possible contrasts.</li>
<li>It should be used if you have not planned contrasts in advance.</li>
<li>For testing pairs of treatment means it is too conservative (you should use Bonferroni or Šídák).</li>
</ul>
</div>
</div>
<div id="lie-detecting-experiment" class="section level1">
<h1>Lie detecting experiment</h1>
<div class="optional-begin">
Lie detectors: Data Codebook<span id="opt-start-171" class="fa fa-plus optional-icon clickable" onclick="toggle_visibility(&#39;opt-body-171&#39;, &#39;opt-start-171&#39;)"></span>
</div>
<div id="opt-body-171" class="optional-body" style="display: none;">
<p>120 participants took part in a study in which they were presented with 100 recordings, and were tasked with guessing whether the speaker in each recording was lying or whether they were telling the truth.<br />
Participants scored points every time they correctly identified a truth or a lie, and lost points whenever they mistook a lie for a truth (or vice versa). The maximum possible points to be scored was 100.</p>
<p>Half of the participants (60) were shown recordings in audio and video, the other half were presented with only the audio track.
Prior to taking the experimen, participants were given material to read for 10 minutes. Half of the participants in each condition (30 in the audio-only condition, and 30 in the audiovideo condition) were given instructional material used by the Police Force to train detectives to pick up on dishonesty during interrogations via various verbal and non-verbal cues. The remaining 30 participants in each condition were given a series of <a href="https://external-preview.redd.it/OY7xRuBBRkx8JFrjYoh3PswM2VMBDHBv4oXbpIEcB3Q.jpg?auto=webp&amp;s=1a12316259fdfd314d4060f44f30260735741da5">cartoon strips to read</a>.</p>
<p>The data is available at <a href="https://uoepsy.github.io/data/lietraining.csv" class="uri">https://uoepsy.github.io/data/lietraining.csv</a></p>
</div>
<p class="optional-end">
</p>
<blockquote>
<p>Research Questions</p>
<p>Do the Police training materials and the mode of communication (audio vs audiovideo) interact to influence the accuracy of veracity judgements?</p>
</blockquote>
<div class="question-begin">
Question 7
</div>
<div class="question-body">
<p>Load the data.<br />
Produce a descriptives table for the variables of interest.
Produce a plot showing the mean points for each condition.</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-172" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-172&#39;, &#39;sol-start-172&#39;)"></span>
</div>
<div id="sol-body-172" class="solution-body" style="display: none;">
<pre class="r"><code>liedat &lt;- read_csv(&quot;https://uoepsy.github.io/data/lietraining.csv&quot;)

liedat$audiovideo &lt;- factor(liedat$audiovideo, labels=c(&quot;audio&quot;,&quot;audio+video&quot;))
liedat$trained &lt;- factor(liedat$trained, labels = c(&quot;untrained&quot;,&quot;trained&quot;))


liestats &lt;- 
  liedat %&gt;% group_by(audiovideo, trained) %&gt;%
  summarise(meanpoints = mean(points),
            se = sd(points)/sqrt(n())
  )

ggplot(liestats, aes(x = audiovideo, y = meanpoints, color = trained)) + 
  geom_point(size = 3) +
  geom_linerange(aes(ymin = meanpoints - 2 * se, ymax = meanpoints + 2 * se)) +
  geom_path(aes(x = as.numeric(audiovideo)))</code></pre>
<p><img src="13_multiplecomp_files/figure-html/unnamed-chunk-21-1.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question 8
</div>
<div class="question-body">
<p>Conduct a two-way ANOVA to investigate research question above.<br />
Be sure to check the assumptions!</p>
<p>Write up your results in a paragraph.</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-173" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-173&#39;, &#39;sol-start-173&#39;)"></span>
</div>
<div id="sol-body-173" class="solution-body" style="display: none;">
<pre class="r"><code>lie_mdl &lt;- lm(points ~ audiovideo * trained, data = liedat)</code></pre>
<pre class="r"><code>plot(lie_mdl)</code></pre>
<p><img src="13_multiplecomp_files/figure-html/unnamed-chunk-24-1.png" width="60%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>shapiro.test(residuals(lie_mdl))</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(lie_mdl)
## W = 0.99122, p-value = 0.6469</code></pre>
<pre class="r"><code>car::ncvTest(lie_mdl)</code></pre>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 2.138705, Df = 1, p = 0.14362</code></pre>
<pre class="r"><code>anova(lie_mdl)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: points
##                     Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## audiovideo           1 1957.7 1957.71  41.263 3.046e-09 ***
## trained              1  545.8  545.85  11.505   0.00095 ***
## audiovideo:trained   1  824.2  824.20  17.372 5.949e-05 ***
## Residuals          116 5503.6   47.45                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="int">
<p>Accuracy of veracity judgements (measured by points scored in lie-detecting game) were analysed with a 2 (audio vs audiovideo) <span class="math inline">\(\times\)</span> 2 (untrained vs trained) between-subjects ANOVA. Residuals did not show significant departure from normality (Shapiro-Wilk <span class="math inline">\(W=0.99, p = .65\)</span>), nor unequal variances across groups (Breusch-Pagan <span class="math inline">\(\chi^2(1)=2.14, p = .14\)</span>). There was a significant interaction between presentation mode and whether or not participants had received training for detecting lies <span class="math inline">\(F(1, 116) = 17.37, p &lt;. 001\)</span>.</p>
</div>
<p><small>
Please note that these are short example write ups that may not be complete for every question you are asked. It is intended to give you a sense of the style.
</small></p>
</div>
<p class="solution-end">
</p>
<div class="question-begin">
Question 9
</div>
<div class="question-body">
<p>Perform a pairwise comparison of the mean accuracy (as measured by points accrued) across the 2×2 factorial design, making sure to adjust for multiple comparisons by the method of your choice.</p>
<p>Write up your results in a paragraph. Combined with your plot of group means, what do you feel about the Police training materials on using behavioural cues to detect lying?</p>
<p class="question-end">
</p>
</div>
<div class="solution-begin">
Solution <span id="sol-start-174" class="fa fa-plus solution-icon clickable" onclick="toggle_visibility(&#39;sol-body-174&#39;, &#39;sol-start-174&#39;)"></span>
</div>
<div id="sol-body-174" class="solution-body" style="display: none;">
<pre class="r"><code>emms_lie &lt;- emmeans(lie_mdl, ~ audiovideo * trained)
lie_con &lt;- contrast(emms_lie, method = &quot;pairwise&quot;, adjust=&quot;tukey&quot;)
lie_con</code></pre>
<pre><code>##  contrast                                        estimate   SE  df t.ratio
##  audio untrained - (audio+video untrained)          2.837 1.78 116  1.595 
##  audio untrained - audio trained                   -0.976 1.78 116 -0.549 
##  audio untrained - (audio+video trained)           12.344 1.78 116  6.941 
##  (audio+video untrained) - audio trained           -3.813 1.78 116 -2.144 
##  (audio+video untrained) - (audio+video trained)    9.507 1.78 116  5.346 
##  audio trained - (audio+video trained)             13.320 1.78 116  7.489 
##  p.value
##  0.3855 
##  0.9467 
##  &lt;.0001 
##  0.1456 
##  &lt;.0001 
##  &lt;.0001 
## 
## P value adjustment: tukey method for comparing a family of 4 estimates</code></pre>
<pre class="r"><code>confint(lie_con)</code></pre>
<pre><code>##  contrast                                        estimate   SE  df lower.CL
##  audio untrained - (audio+video untrained)          2.837 1.78 116    -1.80
##  audio untrained - audio trained                   -0.976 1.78 116    -5.61
##  audio untrained - (audio+video trained)           12.344 1.78 116     7.71
##  (audio+video untrained) - audio trained           -3.813 1.78 116    -8.45
##  (audio+video untrained) - (audio+video trained)    9.507 1.78 116     4.87
##  audio trained - (audio+video trained)             13.320 1.78 116     8.68
##  upper.CL
##     7.473
##     3.660
##    16.980
##     0.823
##    14.143
##    17.956
## 
## Confidence level used: 0.95 
## Conf-level adjustment: tukey method for comparing a family of 4 estimates</code></pre>
<pre class="r"><code>plot(lie_con)</code></pre>
<p><img src="13_multiplecomp_files/figure-html/unnamed-chunk-27-1.png" width="60%" style="display: block; margin: auto;" /></p>
<div class="int">
<p>Tukey’s Honestly Significant Difference comparisons indicated that, contrary to what one might expect, participants who were presented with audiovisual recordings scored on average 9.5 points lower when they had read the police training materials compared to when they had received no training (95% CI [4.87 — 14.14]).
The presentation mode (audio vs audiovideo) was not found to result in a significantly different average score for those who were untrained (95% CI [-1.80 — 7.47]), and nor did training appear to have any effect on detecting lies in the audio-only condition (95% CI [-5.61 — 3.66]).</p>
<p>The findings indicate that, on the whole, people are bad at determining whether they are being presented with a lie or the truth, as the overall mean score was 49.2 out of 100 (SD = 8.61), where a series of completely random guesses expected to score 50/100. The police training materials did not appear to improve lie detection, in fact with trained participants performing worse (compared to untrained) in the audiovisual condition. This may indicate that the training materials focus too heavily on visual cues, which perhaps are not actually associated with dishonesty in the appropriate way.</p>
</div>
<p><small>
Please note that these are short example write ups that may not be complete for every question you are asked. It is intended to give you a sense of the style.
</small></p>
</div>
<p class="solution-end">
</p>
<!-- Formatting -->
<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;">

</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>what defines a ‘family’ of tests is debateable.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

<link rel="stylesheet" href="https://uoepsy.github.io/assets/css/ccfooter.css" />
<div class="ccfooter"></div>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
